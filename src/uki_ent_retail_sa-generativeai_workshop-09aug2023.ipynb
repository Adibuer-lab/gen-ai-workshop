{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a5278e-4064-4cbb-989d-33228b6e0e49",
   "metadata": {},
   "source": [
    "# <div> <b> UKIR ENT Retail SA Workshop - 09August2023 - Generative AI </b> </div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b93e1-b222-449b-aa18-cc00c90c09aa",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Use the notebook environment: PyTorch 1.13  Python 3.8 GPU Optimized | ml.g4dn.xlarge\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6703f8-906c-4ee4-baff-77388ec2d240",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<h1><b> TODO: RS - Update the section </b></h1>\n",
    "\n",
    "<h2>\n",
    "   Brief Description of the notebook and agenda with links to each section \n",
    "    \n",
    "</h2>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588c6c0-cc07-491e-aeed-51e3ad8cbb23",
   "metadata": {},
   "source": [
    "## SetUp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e4af2-a3e5-4731-b98b-fc2c6fc2b075",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> \n",
    "    - Install required core packages used in rest of the sections of the notebook.\n",
    "    - Set global parameters required for the lab.\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19af01cf-4539-488b-a5e8-0d92e372858d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a4a9de-3421-4355-a00b-de133ce65326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1aad71b-7d89-45c7-965a-6437c0623aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = 'huggingface-text2text-flan-t5-xl'  # this is the default model for this lab\n",
    "MODEL_VERSION = '*'\n",
    "INF_INSTANCE_TYPE = 'ml.g5.2xlarge'\n",
    "INF_INSTANCE_COUNT = 1\n",
    "INF_IMAGE_SCOPE = 'inference'\n",
    "TRN_INSTANCE_TYPE = 'ml.g5.12xlarge'\n",
    "TRN_INSTANCE_COUNT = 1\n",
    "TRN_IMAGE_SCOPE = 'training'\n",
    "MODEL_DATA_DOWNLOAD_TIMEOUT = 3600  # in seconds\n",
    "CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT = 3600\n",
    "EBS_VOLUME_SIZE = 256  # in GB\n",
    "CONTENT_TYPE = 'application/json'\n",
    "MODEL_ENDPOINT_PREFIX = 'uki-ent-ret-sa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09414d2c-1b8f-4daf-92d8-96e19bf7ea62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51559a9f-22ed-4033-89bb-d62d314d7d82",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 1\n",
    "---\n",
    "<h1><b> TODO: RS - Update the section </b></h1>\n",
    "\n",
    "<h2>\n",
    "   Brief Description of the notebook and agenda with links to each section \n",
    "\n",
    "   Deploy the state-of-the-art pre-trained model **[FLAN T5 models](https://huggingface.co/docs/transformers/model_doc/flan-t5)** and query the endpoint to generate response from the base model.\n",
    "    \n",
    "</h2>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782384d-6679-4ac9-a49a-13da7189d53a",
   "metadata": {},
   "source": [
    "### **STEPS**\n",
    "- [1.a Select a model](#1.a-Select-a-model)\n",
    "- [1.b Retrieve Artifacts & Deploy an Endpoint](#1.b-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "- [1.c Query endpoint and parse response](#1.c-Query-endpoint-and-parse-response)\n",
    "- [1.d Advanced features: How to use various parameters to control the generated text](#1.d-Advanced-features:-How-to-use-various-advanced-parameters-to-control-the-generated-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a862e1-ddc9-470c-a6c6-a11272b8fea9",
   "metadata": {},
   "source": [
    "### **1.a Select a pre-trained model**\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [SageMaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074fb133-286d-41ce-b36c-be43822ae290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Retrieves all Text Generation models available by SageMaker Built-In Algorithms.\n",
    "filter_value = \"task == text2text\"\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=text_generation_models,\n",
    "    value=MODEL_ID,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa9823a-07f7-4f0b-b000-a03ee541b134",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e6c94558c146a5bff8ca57d0f9dd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f144d67-3f65-44cc-aadd-dc79c1c199d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    The current notebook is only tested against the model : <b> huggingface-text2text-flan-t5-xl </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287971fa-e471-4315-9582-e06a001628e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "MODEL_ID, MODEL_VERSION = model_dropdown.value, \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47534cab-6f80-44ad-9eec-366b14413596",
   "metadata": {},
   "source": [
    "### **1.b Retrieve Artifacts & Deploy an Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b494f8-637a-4866-a358-b65bd15ed18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a unique endpoint name for the current model deployment. Add current timestamp as the suffix if needed\n",
    "base_endpoint_name = f'{MODEL_ENDPOINT_PREFIX}-base-{MODEL_ID}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d62b07-0e17-4291-ad75-e3e46a5684c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=INF_IMAGE_SCOPE,\n",
    "    model_id=MODEL_ID,\n",
    "    model_version=MODEL_VERSION,\n",
    "    instance_type=INF_INSTANCE_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307e52c9-7194-4782-b848-4257315cadf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=MODEL_ID, model_version=MODEL_VERSION, model_scope=INF_IMAGE_SCOPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180d7e1-65a9-43d3-b2fa-4e1ea26c3a28",
   "metadata": {},
   "source": [
    "#### **huggingface-text2text-flan-t5-xl** is already packed with the inference script and model artifacts, so the `source_dir` argument and entryPoint script to the Model are not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a755bc99-7cae-44f7-ad76-5d0a204de575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=base_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3796f3cd-5438-4185-b3e1-aa9610feb791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: uki-ent-ret-sa-base-huggingface-text2text-flan-t5-xl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "# deploy the Model. TODO\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=INF_INSTANCE_COUNT,\n",
    "    instance_type=INF_INSTANCE_TYPE,\n",
    "    endpoint_name=base_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1fe9a-5c90-43fe-88a7-66dcad5224c3",
   "metadata": {},
   "source": [
    "### **1.c Query endpoint and parse response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03f62ff-327c-4991-8722-1b03fcbf15d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-text\", Body=encoded_text\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "594cce31-7515-4a66-b06b-3affe843ba46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate to German:  My name is SageMaker Jumpstart\n",
      "generated text: \u001b[1mIch bin SageMaker Jumpstart.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: A step by step guide to deploy a large language model:\n",
      "generated text: \u001b[1mStep 1: Create a large language model. Step 2: Create a large language model\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "text1 = \"Translate to German:  My name is SageMaker Jumpstart\"\n",
    "text2 = \"A step by step guide to deploy a large language model:\"\n",
    "\n",
    "\n",
    "for text in [text1, text2]:\n",
    "    query_response = query_endpoint(text.encode(\"utf-8\"), endpoint_name=base_endpoint_name)\n",
    "    generated_text = parse_response(query_response)\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"input text: {text}{newline}\"\n",
    "        f\"generated text: {bold}{generated_text}{unbold}{newline}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f883a-60c1-43bf-87d0-8946735a6564",
   "metadata": {},
   "source": [
    "### **1.d. Advanced features: How to use various advanced parameters to control the generated text**\n",
    "\n",
    "***\n",
    "This model also supports many advanced parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **num_return_sequences:** Number of output sequences returned. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **seed:** Fix the randomized state for reproducibility. If specified, it must be an integer.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd2a803f-bce9-41ca-80f9-8754e18769c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Create an AWS account. Navigate to the AWS Console. Click on the Create EC2 instance link. Enter the required information. Click on Create.']\n"
     ]
    }
   ],
   "source": [
    "# Input must be a json\n",
    "payload = {\n",
    "    \"text_inputs\": \"Tell me the steps to create an ec2 instance on AWS cloud\",\n",
    "    \"max_length\": 100,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 20,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\":0.8\n",
    "}\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=base_endpoint_name\n",
    ")\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83964f8-e127-4981-b1f2-b716c1888fd2",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Text Summarization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3ce79ef-0bdc-4661-96f4-70d010ccbe2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software. With CodeWhisperer, you can write a comment in natural language that outlines a specific task in English, such as “Upload a file with server-side encryption.” Based on this information, CodeWhisperer recommends one or more code snippets directly in the IDE that can accomplish the task. You can quickly and easily accept the top suggestion (tab key), view more suggestions (arrow keys), or continue writing your own code. You should always review a code suggestion before accepting them, and you may need to edit it to ensure it does exactly what you intended. CodeWhisperer helps accelerate software development by providing code suggestions that reduce total development effort and allow more time for ideation, complex problem solving, and writing differentiated code. In addition to general purpose code suggestions, CodeWhisperer has additional training to provide code suggestions for using AWS APIs. CodeWhisperer can also help you improve application security by helping detect and remediate security vulnerabilities. As you are writing code, CodeWhisperer analyzes the English language comments and surrounding code to infer what code is needed to complete the task at hand. CodeWhisperer suggests one or more code snippets directly in the code editor, accelerating you as you code. The code suggestions provided by CodeWhisperer are based on a large language models (LLMs) trained on billions of lines of code, including Amazon and open-source code. You can quickly and more easily accept the top suggestion (tab key), view more suggestions (arrow keys), or continue writing your own code. Always review a code suggestion before accepting it, and you may need to edit it to ensure that it does exactly what you intended.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e33527e-c61e-4414-a1b1-707fc831ad4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of return sequences are set as 1\u001b[0m\n",
      "\n",
      "\u001b[1m For prompt: 'Briefly summarize this sentence: {text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software\n",
      "\n",
      "\u001b[1m For prompt: 'Write a short summary for this text: {text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software.\n",
      "\n",
      "\u001b[1m For prompt: 'Generate a short summary this sentence:\n",
      "{text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software. With CodeWhisperer\n",
      "\n",
      "\u001b[1m For prompt: '{text}\n",
      "\n",
      "Write a brief summary in a sentence or less'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software.\n",
      "\n",
      "\u001b[1m For prompt: '{text}\n",
      "Summarize the aforementioned text in a single phrase.'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software.\n",
      "\n",
      "\u001b[1m For prompt: '{text}\n",
      "Can you generate a short summary of the above paragraph?'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software. With CodeWhisperer\n",
      "\n",
      "\u001b[1m For prompt: 'Write a sentence based on this summary: {text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Learn about CodeWhisperer, an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software.\n",
      "\n",
      "\u001b[1m For prompt: 'Write a sentence based on '{text}''\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: CodeWhisperer is an AI coding companion that generates real-time, single-line or full-function code suggestions in your Integrated Development Environment (IDE) to help you quickly build software. With CodeWhisperer\n",
      "\n",
      "\u001b[1m For prompt: 'Summarize this article:\n",
      "\n",
      "{text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 1 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Defining your tasks with natural language. Using CodeWhisperer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Briefly summarize this sentence: {text}\",\n",
    "    \"Write a short summary for this text: {text}\",\n",
    "    \"Generate a short summary this sentence:\\n{text}\",\n",
    "    \"{text}\\n\\nWrite a brief summary in a sentence or less\",\n",
    "    \"{text}\\nSummarize the aforementioned text in a single phrase.\",\n",
    "    \"{text}\\nCan you generate a short summary of the above paragraph?\",\n",
    "    \"Write a sentence based on this summary: {text}\",\n",
    "    \"Write a sentence based on '{text}'\",\n",
    "    \"Summarize this article:\\n\\n{text}\",\n",
    "]\n",
    "\n",
    "num_return_sequences = 1\n",
    "parameters = {\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "print(f\"{bold}Number of return sequences are set as {num_return_sequences}{unbold}{newline}\")\n",
    "for each_prompt in prompts:\n",
    "    payload = {\"text_inputs\": each_prompt.replace(\"{text}\", text), **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=base_endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} For prompt: '{each_prompt}'{unbold}{newline}\")\n",
    "    print(f\"{bold} The {num_return_sequences} summarized results are{unbold}:{newline}\")\n",
    "    for idx, each_generated_text in enumerate(generated_texts):\n",
    "        print(f\"{bold}Result {idx}{unbold}: {each_generated_text}{newline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fcc23-f800-4986-84a6-c5505c624a7d",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Question and Answering**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0688ddfb-1e0e-4157-bb93-038ec02ede90",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"The newest and most innovative Kindle yet lets you take notes on millions of books and documents, write lists and journals, and more. \n",
    "\n",
    "For readers who have always wished they could write in their eBooks, Amazon’s new Kindle lets them do just that. The Kindle Scribe is the first Kindle for reading and writing and allows users to supplement their books and documents with notes, lists, and more.\n",
    "\n",
    "Here’s everything you need to know about the Kindle Scribe, including frequently asked questions.\n",
    "\n",
    "The Kindle Scribe makes it easy to read and write like you would on paper \n",
    "\n",
    "The Kindle Scribe features a 10.2-inch, glare-free screen (the largest of all Kindle devices), crisp 300 ppi resolution, and 35 LED front lights that automatically adjust to your environment. Further personalize your experience with the adjustable warm light, font sizes, line spacing, and more.\n",
    "\n",
    "It comes with your choice of the Basic Pen or the Premium Pen, which you use to write on the screen like you would on paper. They also attach magnetically to your Kindle and never need to be charged. The Premium Pen includes a dedicated eraser and a customizable shortcut button.\n",
    "\n",
    "The Kindle Scribe has the most storage options of all Kindle devices: choose from 8 GB, 16 GB, or 32 GB to suit your level of reading and writing.\n",
    "\"\"\"\n",
    "question = \"what are the key features of new Kindle?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74c6973f-06df-4be8-81f0-426e88b41217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m The reasoning result is\u001b[0m: '['Kindle Scribe']'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['10.2-inch, glare-free screen']'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['The Kindle Scribe features a 10.2-inch, glare-free screen (the largest of all Kindle devices), crisp 300 ppi resolution, and 35 LED front lights that automatically adjust to your environment']'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['The Kindle Scribe is the first Kindle for reading and writing and allows users to supplement their books and documents with notes, lists, and more.']'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['the large screen']'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['1.']'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['More than 60 percent of customers say they are excited to use the Kindle Scribe.']'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\",\n",
    "    \"\"\"{context}\\n\\nAnswer this question based on the article: {question}\"\"\",\n",
    "    \"\"\"{context}\\n\\n{question}\"\"\",\n",
    "    \"\"\"{context}\\nAnswer this question: {question}\"\"\",\n",
    "    \"\"\"Read this article and answer this question {context}\\n{question}\"\"\",\n",
    "    \"\"\"{context}\\n\\nBased on the above article, answer a question. {question}\"\"\",\n",
    "    \"\"\"Write an article that answers the following question: {question} {context}\"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{context}\", context)\n",
    "    input_text = input_text.replace(\"{question}\", question)\n",
    "    #print(f\"{bold} For prompt{unbold}: '{each_prompt}'{newline}\")\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=base_endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c8cf-8268-49da-8f3a-9c053fcb26b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### **Imaginary article generation based on a title**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55d67949-e573-42d1-8199-b29c2129ba47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title = \"AnyCompany business has a new product category coming up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b8b7309-544f-45f1-9412-cb8d531a6ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m The reasoning result is\u001b[0m: '['There’s always been a smattering of people talking about the products that AnyCompany has to offer. There is, after all, an entire online marketplace of companies, some of which are known for their expertise in one particular industry.']'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"Title: \\\"{title}\\\"\\\\nGiven the above title of an imaginary article, imagine the article.\\\\n\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 5000,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{title}\", title)\n",
    "    #print(f\"{bold} For prompt{unbold}: '{input_text}'{newline}\")\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=base_endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b4c47-b309-450b-a04c-f72f192fe8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d30bce-1059-4016-a1ec-d1e5b0bce562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6733a6-7d02-4126-bfb3-db85e000c125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf9099-4c34-45a3-87ca-c286c2b72c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23926d-40c3-47f7-b3e4-07c28f2625c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dde65a2f-dec9-4421-957b-bf9ae719c519",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "---\n",
    "<h1><b> TODO: RS - Update the section </b></h1>\n",
    "\n",
    "<h2>\n",
    "   Brief Description of the notebook and agenda with links to each section \n",
    "    <br/>\n",
    "\n",
    "   - **Implement RAG** \n",
    "    \n",
    "</h2>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832d7de-b6d3-4737-92cf-9f0e12c85b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e8d33-06a4-4faa-b529-f887d33ec86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f44b96-d882-4d3e-a699-3b62c3ae2a59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9eed96-771b-40f6-81c5-4e70171651e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568041ec-0ae4-4fe8-be31-11c553219ac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 3\n",
    "---\n",
    "<h1><b> TODO: RS - Update the section </b></h1>\n",
    "\n",
    "<h2>\n",
    "   Brief Description of the notebook and agenda with links to each section \n",
    "\n",
    "   - Explore Prompt Engineering Techniques\n",
    "    \n",
    "</h2>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf52c4a-dbf3-4e0c-871d-3a85f90470bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78b8e8-3874-435b-91bf-3b7f9d074524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772af38-0265-4609-9d89-9a4fd34578a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d01b3-5ca8-4ef4-a65c-d79a295d0f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac876030-4ab2-4f37-a7ec-156f97ed14af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccaa6e15-82fc-44d3-9162-913c0427d682",
   "metadata": {},
   "source": [
    "# Section 4\n",
    "---\n",
    "# **TODO: RS - Update the section** #\n",
    "\n",
    "##   - **Transfer Learning for Domain Adaptation** ##\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d5f1d-5019-49b3-8caa-d901f1047311",
   "metadata": {},
   "source": [
    "## **TBR - RS**\n",
    "### 4.1. Retrieve Training artifacts\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the latest model. Also, we do need to specify the training_instance_type to fetch train_image_uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cad0ee-badb-48bb-8a00-b5895800c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=MODEL_ID,\n",
    "    model_version=MODEL_VERSION,\n",
    "    image_scope=TRN_IMAGE_SCOPE,\n",
    "    instance_type=TRN_INSTANCE_TYPE,\n",
    ")\n",
    "\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=MODEL_ID, model_version=MODEL_VERSION, script_scope=TRN_IMAGE_SCOPE\n",
    ")\n",
    "\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=MODEL_ID, model_version=MODEL_VERSION, model_scope=TRN_IMAGE_SCOPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4cff7-1bc6-49c9-a77c-6791dc9872f7",
   "metadata": {},
   "source": [
    "### 4.2. Set Training parameters\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Text Classification model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. \n",
    "\n",
    "The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. \n",
    "***\n",
    "The second set of parameters are algorithm specific training hyper-parameters. It is also used for sepcifying the model name if we want to fine-tune on the model which is not present in the dropdown list.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738c327-e70e-4eac-b814-012566bfd7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed8bd5-69ef-4392-b75b-87839d847cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43dc791-b92b-4719-8292-0d1e630d0e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52cb7f-941e-4e44-849b-b7fba1a78f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03101c46-ecac-4339-862e-c204f52bd8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b998d-4e36-4e02-9bae-e0402a3e166f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349afdf8-090d-4b22-96ad-cec763bfcd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019537eb-af69-4811-a0d3-47a2f0e56b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa83ef8-3dd0-4b1d-a318-6c27f8b34db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deba046f-b03d-48a9-8532-a74171196da9",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a0e18-d996-4839-991c-732dd8c398ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> IMPORTANT: Clean Up the resources to aviod charges for the resources in use. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede525b-6d60-4b65-88fa-82efee413885",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "- Delete the endpoints for all the deployed models\n",
    "- Delete the base model image. You can choose to not delete the trained/fine-tuned models from S3 so that you can redeploy them in future. Be aware of the storage charges involved\n",
    "- Shutdown the kernel of this notebook and any active kernels on the other notebooks *(Check the running terminals and kernels icon in the left navigation of SageMaker studio)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a5b41-7337-4578-9ea0-d11f5a5c50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model - Delete the SageMaker endpoint and the model stored on S3\n",
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37806df7-ba8c-477f-99b6-35643f046715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Model - Delete the SageMaker endpoint and the model stored on S3\n",
    "# peft_model_predictor.delete_model() # Optional\n",
    "peft_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439c69b-1032-4507-848e-2e6c7ab1f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Model - Delete the SageMaker endpoint and the model stored on S3\n",
    "# train_model_predictor.delete_model() # Optional\n",
    "train_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76300aec-f665-4c42-ac68-3ddd8bf15a6d",
   "metadata": {},
   "source": [
    "## Release the Notebook Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f98bd-e909-4982-aa45-65dd6853a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a020a-4a44-43c3-a142-abdbc9155ee0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>You have successfully completed the lab session. Do not forget to share your feedback through the below survey. </b>\n",
    "</div>\n",
    "\n",
    "### **TODO :: - Pulse Survey Link**\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-gpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
