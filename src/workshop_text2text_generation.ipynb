{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6860380-ddd6-4fb7-9fc8-a765049cde01",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": []
   },
   "source": [
    "# Workshop: Text2Text Generation with SageMaker\n",
    "\n",
    "Welcome to this workshop on Text2Text Generation with SageMaker. In this workshop, we will be using a pre-trained model deployed on a SageMaker endpoint to perform text-to-text generation tasks.\n",
    "The workshop is divided into several sections:\n",
    "\n",
    "1. **Setting up the environment:** In this section, we will import necessary libraries and define some helper functions.\n",
    "2. **Querying the endpoint:** We will define some example input texts and use them to query the SageMaker endpoint.\n",
    "3. **Advanced features:** We will explore some advanced features of the model, such as controlling the length of the generated text and the number of output sequences returned.\n",
    "3. **Prompt Engineering:** We will explore some prompt engineering tactics\n",
    "4. **RAG with FAISS:** We will use the langchain library to create a question answering chain and perform similarity searches on a set of documents.\n",
    "5. **Cleaning up:** Finally, we will shut down the SageMaker endpoint to avoid incurring unnecessary costs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c8f25-e6b4-4e23-ab26-130b14b961ad",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": []
   },
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "In this section, we will import the necessary libraries and define some helper functions that we will use throughout the workshop.\n",
    "\n",
    "We will be using the `json` and `boto3` libraries. The `json` library provides functions for working with JSON data, and the `boto3` library allows us to interact with AWS services, including SageMaker.\n",
    "\n",
    "Let's start by importing these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d97202-0ccb-4c46-bc4c-2dd59522fbc5",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fa874-75f8-409e-a3bd-9ca52046f8fe",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define some example input texts. These are the texts that we will use to query the SageMaker endpoint. The model will take these texts as input and return the output of the accomplished task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5de5533-057c-42c7-9bb1-b82fcc597d0d",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = \"Translate to German:  My name is Arthur\"\n",
    "text2 = \"A step by step recipe to make bolognese pasta:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed01cc6-5788-4c56-b922-510b79954172",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's define the endpoint that you have created. We will use this endpoint to query the model and get the generated text. We will also define some formatting variables for better output visualization.\n",
    "\n",
    "The `endpoint_name` variable should be set to the name of the SageMaker endpoint that you have created. The `newline`, `bold`, and `unbold` variables are used to format the output text for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989560f3-0e09-46c0-91a8-dda423fe14f1",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "endpoint_name = 'jumpstart-dft-hf-text2text-flan-t5-xxl'\n",
    "embedding_endpoint_name = 'jumpstart-dft-hf-textembedding-gpt-j-6b-fp16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383d993-92d1-453e-9c65-c5844f1e8ca1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define a function to query the endpoint. This function will take the encoded text as input and return the response from the endpoint.\n",
    "\n",
    "The `query_endpoint` function uses the `boto3` library to create a SageMaker runtime client. It then uses this client to invoke the SageMaker endpoint with the encoded text as input. The function returns the response from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7899e9d-2717-496c-bba5-843c11305be0",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint(encoded_text):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-text', Body=encoded_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12271244-7b57-4e39-800a-07f033c7c810",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will also define a function to parse the response from the endpoint. This function will extract the generated text from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6717efa9-3cc6-48fe-8c71-c0efc1df3359",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions['generated_text']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e931f-fb93-4de4-b7f9-be4a8ccaf4e8",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's use these functions to query the endpoint with our example texts and print the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317c492f-114f-4af6-82b2-c8da833739d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    query_response = query_endpoint(prompt.encode('utf-8'))\n",
    "    generated_text = parse_response(query_response)\n",
    "    print (f\"Inference:{newline}\"\n",
    "            f\"input text: {text}{newline}\"\n",
    "            f\"generated text: {bold}{generated_text}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ddaeefc-95e6-4a41-943f-d54c942c2c47",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate to German:  My name is Arthur\n",
      "generated text: \u001b[1mIch bin Arthur.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: A step by step recipe to make bolognese pasta:\n",
      "generated text: \u001b[1mAdd the ground beef to a large skillet and cook over medium heat until browned, about\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in [text1, text2]:\n",
    "    get_completion(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2ad49-201f-4334-a27b-e62b9ba9f2c2",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": []
   },
   "source": [
    "### Advanced Features\n",
    "\n",
    "The model we are using supports many advanced parameters that can be used to control the text generation process. These parameters include:\n",
    "\n",
    "- **max_length:** This parameter controls the maximum length of the generated text. The model will generate text until the output length (which includes the input context length) reaches `max_length`.\n",
    "- **num_return_sequences:** This parameter controls the number of output sequences returned by the model.\n",
    "- **num_beams:** This parameter controls the number of beams used in the greedy search during text generation.\n",
    "- **no_repeat_ngram_size:** This parameter ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence.\n",
    "- **temperature:** This parameter controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words.\n",
    "- **early_stopping:** If set to True, text generation is finished when all beam hypotheses reach the end of sentence token.\n",
    "- **do_sample:** If set to True, the model will sample the next word as per the likelihood.\n",
    "- **top_k:** In each step of text generation, the model will sample from only the `top_k` most likely words.\n",
    "- **top_p:** In each step of text generation, the model will sample from the smallest possible set of words with cumulative probability `top_p`.\n",
    "- **seed:** This parameter can be used to fix the randomized state for reproducibility.\n",
    "\n",
    "We can specify any subset of these parameters when invoking the endpoint. In the next section, we will show an example of how to invoke the endpoint with these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faacecfc-a9d6-44c2-ae23-89888ddb54c4",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":\"Tell me the steps to make a pizza\", \"max_length\":50, \"num_return_sequences\":3, \"top_k\":50, \"top_p\":0.95, \"do_sample\":True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de10c2-ff8a-4d04-b51e-c57ebb4237b6",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now define a function to query the endpoint with a JSON payload. This function will take the encoded JSON as input and return the response from the endpoint.\n",
    "\n",
    "The `query_endpoint_with_json_payload` function is similar to the `query_endpoint` function we defined earlier. The difference is that this function takes a JSON payload as input instead of a text. This allows us to pass the advanced parameters to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e974115-528c-4520-80bd-df98a531efde",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab43614-a39c-42f8-8301-a260b61a60df",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will also define a function to parse the response from the endpoint when multiple texts are returned. This function will extract the generated texts from the response.\n",
    "\n",
    "The `parse_response_multiple_texts` function is similar to the `parse_response` function we defined earlier. The difference is that this function extracts the 'generated_texts' field from the JSON instead of the 'generated_text' field. This is because when we request multiple texts from the endpoint, the response contains a 'generated_texts' field with a list of generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf4a45b-c164-463b-b372-b3fb0b0f0594",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions['generated_texts']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7420d-affe-4f44-b67d-074e459b26eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's use these functions to query the endpoint with our JSON payload and print the generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7354401b-c3b1-40b0-ac61-5d4068d965e1",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To make a pizza, you need a pizza stone, pizza dough, pizza sauce, cheese, and toppings. To make a pizza, you need a pizza stone, pizza dough, pizza sauce, cheese, and toppings', 'To make a pizza, you need a pizza pan, pizza sauce, cheese, and toppings.', '1 tbsp olive oil 2 tbsp pizza sauce 3 tbsp grated mozzarella cheese 4 tbsp grated Parmesan cheese 5 tbsp grated Romano cheese']\n"
     ]
    }
   ],
   "source": [
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38b2a6f6-a946-46e9-851f-d35c05797e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_query_endpoint(query):\n",
    "    payload = {\n",
    "        \"text_inputs\": query,\n",
    "        \"max_length\": 5000,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.01\n",
    "    }\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=json.dumps(payload).encode('utf-8'))\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_completion(query):\n",
    "    return parse_response_multiple_texts(\n",
    "        my_query_endpoint(query)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0bf0b-157d-4479-b021-12671547defa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c96d9-6293-4e03-97af-5dd6d0bcce64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prompting Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a6b71-88f9-43ac-bb5e-919772b0fde0",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Write clear and specific instructions.\n",
    "2. Give the model time to “think”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c971114-cf3d-4bec-9b91-bb20a2a4901f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tactics for 'Write clear and specific instructions'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48caea10-ca2a-4cf5-a831-65e979a05763",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1756c6-6138-446e-868c-8a88921cbf7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Write clear prompts.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "delimiter_prompt = f\"Summarize the text delimited by triple backticks\\into a single sentence.\\n```{text}```\"\n",
    "\n",
    "get_completion(delimiter_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05989d1-0c39-4ea9-a677-6a19bfd7d9e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 2: Ask for a structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890e2768-c0a9-42ff-ad92-3d059c404a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[['The Chronicles of Imaginia', 'Love Among the Stars', 'Murder at Midnight Manor']]\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''Generate a list of 3 fictional book information and write them in the following JSON format:\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"book_id\": 1, \n",
    "    \"title\": \"The Chronicles of Imaginia\",\n",
    "    \"author\": \"Phoebe Imaginaire\", \n",
    "    \"genre\": \"Fantasy\"\n",
    "  },\n",
    "  {\n",
    "    \"book_id\": 2,\n",
    "    \"title\": \"Love Among the Stars\",\n",
    "    \"author\": \"Stella Cosmos\",\n",
    "    \"genre\": \"Science Fiction Romance\"   \n",
    "  },\n",
    "  {\n",
    "    \"book_id\": 3,\n",
    "    \"title\": \"Murder at Midnight Manor\",\n",
    "    \"author\": \"I.M. Mysterious\",\n",
    "    \"genre\": \"Mystery\"\n",
    "  }\n",
    "]\n",
    "'''\n",
    "\n",
    "get_completion(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d636b-540e-4e25-9a08-836bccc3911b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbeb259e-a335-4850-b607-264b9e8cb5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sun is out today. The sun is out today. The sun is out today.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = f\"\"\"The sun is out today.\"\"\"\n",
    "\n",
    "prompt = f'''You will be provided with text delimited by triple quotes. \n",
    "If the text includes a mention of 'rain', write a mysterious, suspenseful story\\\n",
    "involving strange events that takes place on a dark, stormy night.\\\n",
    "If the text includes a mention of 'sun', write a lighthearted, \\\n",
    "feel-good story.\\\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "Story:\n",
    "'''\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcf18f-613f-4b91-ad42-e1eec68544dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_2 = \"\"\"\n",
    "The rain falls constantly\n",
    "\"\"\"\n",
    "\n",
    "prompt = f'''You will be provided with text delimited by triple quotes. \n",
    "If the text includes a mention of 'rain', write a mysterious, suspenseful story\\\n",
    "involving strange events that takes place on a dark, stormy night.\\\n",
    "If the text includes a mention of 'sun', write a lighthearted, \\\n",
    "feel-good story.\\\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "Story:\n",
    "'''\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574d8af-7600-4613-8e81-956c49457677",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 4: \"Few-shot\" prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44263af0-a89a-4e6a-a8be-86050dee4cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grandparent>: The  phoenix rises from the ashes; the  oak grows back from the twig;  the grass grows back from the plowed field.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2f3bc-771f-4945-a40f-aba915ffa242",
   "metadata": {},
   "source": [
    "### Tactics for 'Give the model time to “think”.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1bb38e-ba91-4ea3-bbe9-9ce06ff08322",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38f2cd66-1ac2-4c9b-b6bd-0fdd29867bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completion for prompt 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" Dans une ville charmante, les frères Jack et Jill se sont engagés  à obtenir de l'eau d'une source située  à la cime de la montagne.  Pendant que Jack chantait en joie, une misfortune  s'est produite : Jack a tripping sur un rocher et s'est tué  en boucle, avec Jill en suite.  Même si légèrement bruyées, la paire retournait  à la maison pour des hugs rafraichissants. Malgré le dommage,  leurs esprits aventuriers n'avaient pas été enfouis et  ils ont poursuivi leurs activités d'aventure avec élan. \"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \\ \n",
    "a quest to fetch water from a hilltop \\ \n",
    "well. As they climbed, singing joyfully, misfortune \\ \n",
    "struck—Jack tripped on a stone and tumbled \\ \n",
    "down the hill, with Jill following suit. \\ \n",
    "Though slightly battered, the pair returned home to \\ \n",
    "comforting embraces. Despite the mishap, \\ \n",
    "their adventurous spirits remained undimmed, and they \\ \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "print(\"\\nCompletion for prompt 1:\")\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bf2de-707d-47c9-adeb-10ac11d5184d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ask for output in a specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38117325-7837-4bf4-a760-6abca57e2c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completion for prompt 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Une recettes en étapes pour faire une pasta bolognese :>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in Italian summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "print(\"\\nCompletion for prompt 2:\")\n",
    "get_completion(prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50907102-e86f-41a7-ae17-220ead42b23b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72449e29-8fbb-4c80-84dd-f71997141973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c3bcd-f5bd-4eb1-90c4-d726fa641a67",
   "metadata": {},
   "source": [
    "#### Note that the student's solution is actually not correct.\n",
    "#### We can fix this by instructing the model to work out its own solution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69939c78-3453-42b3-a05c-2264ed2cd19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ff671-3b88-4611-859e-c2fb2f2cc077",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 4: Model Limitations: Hallucinations\n",
    "- Boie is a real company, the product name is not real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16db12bc-86b7-4128-9abd-7d53eb708b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AeroGlide UltraSlim Smart Toothbrush by Boie is a smart toothbrush that uses a built-in camera to detect plaque and gum disease. It also has a built-in timer that allows you to brush for a set amount of time.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
    "\"\"\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2f5b1-362a-4bab-9436-1036be407429",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 5: RAG with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b683e3-0ede-4205-a63d-57b63850a270",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Before we proceed to the next steps, let's ensure that we have the necessary libraries installed. We will need the `langchain` library for the following steps. If it's not already installed, we can install it using pip.\n",
    "\n",
    "The `langchain` library is a Python library that provides utilities for working with large language models. It includes utilities for creating prompts, querying endpoints, parsing responses, and more. We will use this library in the following steps to interact with our SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33298503-afb7-4170-8867-e624c335da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt-get install libmagic-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0efe3-428d-4e12-a38e-b3671ac24736",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade python-magic unstructured langchain faiss-cpu pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63738465-b0bd-48f3-b436-66c5e580b1eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's import some necessary modules from the `langchain` library.\n",
    "\n",
    "- `PromptTemplate`: This class is used to create a template for the prompts that we will pass to the language model. \n",
    "- `SagemakerEndpoint`: This class is used to interact with the SageMaker endpoint.\n",
    "- `LLMContentHandler`: This class is used to handle the content that we send to and receive from the language model.\n",
    "- `load_qa_chain`: This function is used to load a question-answering chain. A chain is a sequence of transformations applied to the input to generate an answer.\n",
    "- `Document`: This class is used to create documents that the language model can use to find the answer to a question.\n",
    "- `EmbeddingsContentHandler`: This class is used to handle the content that we send to and receive from the embedding model.\n",
    "- `SagemakerEndpointEmbeddings`: This class is used to interact with the SageMaker embeddings enpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba22d63-2d2a-462d-a39e-88b5d2a6d1f6",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain, LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dffc3-227c-4269-94a1-b836d8e7ffd1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now create a content handler for the language model to transform input to a format that the SageMaker endpoint expects and output to a form that the language model class expects. We will also define some parameters for the model.\n",
    "\n",
    "The `ContentHandler` class is a subclass of the `LLMContentHandler` class. It defines two methods:\n",
    "\n",
    "- `transform_input`: This method takes a prompt and a dictionary of model parameters as input, and returns the input in a format that the SageMaker endpoint expects. In this case, it converts the input to a JSON string and encodes it to bytes.\n",
    "- `transform_output`: This method takes the output from the SageMaker endpoint and returns it in a form that the language model class expects. In this case, it decodes the output from bytes to a string, parses the JSON, and returns the 'generated_texts' field.\n",
    "\n",
    "The `parameters` dictionary defines the parameters that we will use when querying the language model. These parameters control the behavior of the language model, such as the maximum length of the generated text, the number of sequences to return, and the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d01c2de-8ab3-4c2c-bd6e-a69664744287",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 5000,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.01,\n",
    "}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json['generated_texts'][0]\n",
    "    \n",
    "\n",
    "\n",
    "llm_content_handler = ContentHandler()\n",
    "sm_llm=SagemakerEndpoint(\n",
    "            endpoint_name=endpoint_name,\n",
    "            region_name=\"eu-central-1\",\n",
    "            model_kwargs=parameters,\n",
    "            content_handler=llm_content_handler,\n",
    "        )\n",
    "creative_llm=SagemakerEndpoint(\n",
    "            endpoint_name=endpoint_name,\n",
    "            region_name=\"eu-central-1\",\n",
    "            model_kwargs={\n",
    "                \"max_length\": 5000,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"top_k\": 250,\n",
    "                \"top_p\": 0.95,\n",
    "                \"do_sample\": False,\n",
    "                \"temperature\": 2.5\n",
    "            },\n",
    "            content_handler=llm_content_handler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f130c-b368-4562-b863-6d1f5062cd3d",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define a prompt template and load a chain. \n",
    "\n",
    "The prompt template is used to format the input to the language model. It accepts a set of parameters from the user that can be used to generate a prompt for a language model. \n",
    "\n",
    "The question answering chain is a sequence of transformations applied to the input to generate an answer.\n",
    "\n",
    "The `PromptTemplate` class takes a template string and a list of input variables as arguments. The template string is a string that contains placeholders for the input variables. The placeholders are enclosed in curly braces `{}` and correspond to the names of the input variables. When we use the prompt template, we will replace the placeholders with the actual values of the input variables.\n",
    "\n",
    "The `chain` function loads a chain. A chain is a sequence of transformations applied to the input to generate an answer. Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. In this case, the chain includes the language model and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc35a925-898d-46f3-bab7-fefd896983fc",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "            template=\"Use the following pieces of context to answer the question at the end.\\n{context}\\nQuestion: {question}\\nAnswer:\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "chain = load_qa_chain(\n",
    "        llm=sm_llm,\n",
    "        prompt=prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afa2f4-e7c3-4eb7-8095-6c8ebd9e5ebe",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's test our question answering chain with a sample question and some context. The context is a list of documents that the model can use to find the answer to the question.\n",
    "\n",
    "The `chain` function takes a dictionary as input and returns the output of the chain. The input dictionary must contain the 'input_documents' and 'question' keys. The 'input_documents' key corresponds to a list of documents that the model can use to find the answer to the question. The 'question' key corresponds to the question that we want to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a267d2db-d36a-4d70-a977-d4c26e189db2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Spot instances'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "input_documents = [Document(page_content=\"\")]\n",
    "\n",
    "chain({\"input_documents\": input_documents, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06cb55-e53c-4a89-85a3-ff8128b4a6a1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will create a content handler for embeddings to transform a format that the SageMaker endpoint expects and output to a form that the embeddings class expects.\n",
    "\n",
    "The `SagemakerEndpointEmbeddingsJumpStart` class is a subclass of the `SagemakerEndpointEmbeddings` class. It defines the `embed_documents` method, which computes document embeddings using a SageMaker Inference Endpoint. The method takes a list of texts and a chunk size as input, and returns a list of embeddings.\n",
    "\n",
    "The `ContentHandler` class is a subclass of the `EmbeddingsContentHandler` class. It defines two methods:\n",
    "\n",
    "- `transform_input`: This method takes a prompt and a dictionary of model parameters as input, and returns the input in a format that the SageMaker endpoint expects. In this case, it converts the input to a JSON string and encodes it to bytes.\n",
    "- `transform_output`: This method takes the output from the SageMaker endpoint and returns it in a form that the embeddings class expects. In this case, it decodes the output from bytes to a string, parses the JSON, and returns the 'embedding' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb0f7dc1-6584-4c9e-a2bb-30e465881cf2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "embeddings_content_handler=ContentHandler()\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=\"eu-central-1\",\n",
    "    content_handler=embeddings_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879915e9-6ef5-41cd-a986-3abf044ec7c8",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now we will load the data we will embed for contextual prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82d8140a-c7f4-44fe-a930-207c54539717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.url import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8361d0-c950-4bc7-b2c9-a8d16e9515e2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://aws.amazon.com/codewhisperer/faqs/\",\n",
    "    \"https://aws.amazon.com/sagemaker/faqs/\",\n",
    "]\n",
    "headers={\"ssl_verify\":\"False\"}\n",
    "loader = UnstructuredURLLoader(urls=urls,headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7305ce-64a7-493e-a816-930c443b6670",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now install the `faiss-cpu` library\n",
    "\n",
    "`faiss-cpu` provides efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI that allows for efficient similarity search and clustering of dense vectors. So, given a set of vectors(in this case a vector representation of a document i.e. an embedding), we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c7fb7ce-54a3-4118-bfa7-32f79192f222",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cb248-4acb-4b4d-adbd-54e2f4ec9997",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now create an index of our documents using the VectorstoreIndexCreator. This index will allow us to perform efficient similarity searches on our documents.\n",
    "\n",
    "The VectorstoreIndexCreator is a utility that helps us create an index of our documents. It uses the embeddings of the documents to create the index. The embeddings are dense vectors that represent the documents. The index allows us to perform efficient similarity searches on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0f96ef5-4026-49fc-a0a0-4aaa1f2e9395",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader,CSVLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter = text_splitter\n",
    ")\n",
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcac91-176b-4cf3-8f6b-eeb8cd469751",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Let's test our index by querying it with a sample question.\n",
    "\n",
    "The `index.query` function is used to perform a similarity search on the index. It takes a question and a language model as input, and returns the most similar documents in the index. After the relevant documents are retrieved, the LLM can be used to generate a coherent and contextually relevant answer based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "175c4fc3-a0e9-4379-9832-9e5ebfae187a",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Managed Spot Training can be used with all instances supported in SageMaker.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(question=query, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee42e36-d687-45f1-b0d5-befdf5c16f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will now replicate the index.query functionality step by step to illustrate what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a07a4-5044-4639-8351-be6399eabf53",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "we will create a document search object using the FAISS vector store and our documents. This will allow us to perform similarity searches on our documents. Using this we retrieve the top 3 most similar docs to our query.\n",
    "\n",
    "The `FAISS.from_documents` function is used to create a FAISS vector store from our documents. The embeddings of the documents are used to create the vector store. The vector store allows us to perform efficient similarity searches on the documents.\n",
    "\n",
    "The `docsearch.similarity_search` function is used to perform a similarity search on the documents. It takes a query and a number of results to return as input, and returns the most similar documents in the vector store. The query is converted into an embedding and this embedding is then compared with the embeddings of the documents in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a43072c-1102-400b-b5f7-d4a52b8d614f",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q: Which instances can I use with Managed Spot Training?\\n\\nManaged Spot Training can be used with all instances supported in SageMaker.\\n\\nQ: Which Regions are supported with Managed Spot Training?', metadata={'source': 'https://aws.amazon.com/sagemaker/faqs/', 'start_index': 46752}),\n",
       " Document(page_content='Q: When should I use Managed Spot Training?', metadata={'source': 'https://aws.amazon.com/sagemaker/faqs/', 'start_index': 45014}),\n",
       " Document(page_content='Q: Why should I use SageMaker Serverless Inference?', metadata={'source': 'https://aws.amazon.com/sagemaker/faqs/', 'start_index': 55822})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "splitdocuments = text_splitter.split_documents(documents)\n",
    "docsearch = FAISS.from_documents(splitdocuments, embeddings)\n",
    "docs = docsearch.max_marginal_relevance_search(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffb027-7392-4e41-9fcc-e1b87e96e7ea",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Finally, we will use our question-answering chain to answer our query using the documents we found.\n",
    "\n",
    "The `chain` function is used to apply our question-answering chain to our query and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65db642c-0143-4adb-8b61-4f556966bb76",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'all instances'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec624fcc-68b6-428d-950d-600ff5336a91",
   "metadata": {},
   "source": [
    "We have looked at two flows so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96249b-a493-475f-b239-1aed1025947c",
   "metadata": {},
   "source": [
    "a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144f57a-8c3f-425e-bb66-91897c60b301",
   "metadata": {},
   "source": [
    "![alt text](flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda94971-ef3f-433c-87f9-d1ccbd2d0174",
   "metadata": {},
   "source": [
    "b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b173f-7078-4e73-abd1-e4d0f5719571",
   "metadata": {},
   "source": [
    "![alt text](RAGflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21152ce5-bf72-4c4f-95a8-c6cd9f8b2ae7",
   "metadata": {},
   "source": [
    "Now let's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b9d6e61-3c55-4f6b-aa4b-a11ffae97bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://mysagebucket-4590283737/RAGFiles/retail_items.csv to rag_data/retail_items.csv\n"
     ]
    }
   ],
   "source": [
    "retail_data = \"s3://mysagebucket-4590283737/RAGFiles/\"\n",
    "!aws s3 cp --recursive $retail_data rag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65ec90a7-0f46-4400-8fca-7affdcde4753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/2980811255.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_df['description'] = df.apply(lambda row: f\"{row['name']} is a {row['style']} in the {row['category']} category. Description: {row['description']} with a Price of ${row['price']} and Current stock is {row['current_stock']}.\", axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sans Pareil Scarf is a scarf in the apparel ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chef Knife is a kitchen in the housewares cate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gainsboro Jacket is a jacket in the apparel ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High Definition Speakers is a speaker in the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spiffy Sandals is a sandals in the footwear ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  Sans Pareil Scarf is a scarf in the apparel ca...\n",
       "1  Chef Knife is a kitchen in the housewares cate...\n",
       "2  Gainsboro Jacket is a jacket in the apparel ca...\n",
       "3  High Definition Speakers is a speaker in the e...\n",
       "4  Spiffy Sandals is a sandals in the footwear ca..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('rag_data/retail_items.csv')\n",
    "\n",
    "processed_df=df[['description']]\n",
    "processed_df['description'] = df.apply(lambda row: f\"{row['name']} is a {row['style']} in the {row['category']} category. Description: {row['description']} with a Price of ${row['price']} and Current stock is {row['current_stock']}.\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "processed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b93fe927-91b3-4789-9f68-01da8f12a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[['description']].to_csv(\"rag_data/processed_retail_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7c71116-304f-44b9-a76e-900342fde412",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_data_loader = CSVLoader(file_path=\"rag_data/processed_retail_data.csv\")\n",
    "retail_data_documents = retail_data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ed0c57e-364b-4feb-af52-bc78f54ba0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "retail_data_index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter=text_splitter,\n",
    ")\n",
    "retail_data_index = retail_data_index_creator.from_loaders([retail_data_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d5c7423-4f27-448e-b336-1eaab9c2c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_query=\"What is the price and stock of Sans Pareil scarf?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cba294f8-e525-4bed-b96d-70bc599d3655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Sans Pareil scarf is a limited edition scarf, produced in a limited edition of 450 pieces.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": input_documents, \"question\": retail_query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c51f25d3-4eae-40ac-8814-25773f3b81a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$114.99 and Current stock is 6'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail_data_index.query(question=retail_query, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4f5b7d6-b1bd-4264-841d-cc802c6761e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='description: Sans Pareil Scarf is a scarf in the apparel category. Description: Sans pareil scarf for women with a Price of $114.99 and Current stock is 6.', metadata={'source': 'rag_data/processed_retail_data.csv', 'row': 199}),\n",
       " Document(page_content='description: Set is a set in the tools category. Description: This set is a must-have for your toolbox with a Price of $8.99 and Current stock is 13.', metadata={'source': 'rag_data/processed_retail_data.csv', 'row': 1473}),\n",
       " Document(page_content='description: Rich Soap is a bathing in the beauty category. Description: Enjoy the fragrance of this rich soap with a Price of $73.99 and Current stock is 10.', metadata={'source': 'rag_data/processed_retail_data.csv', 'row': 375})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail_data_docsearch = FAISS.from_documents(retail_data_documents, embeddings)\n",
    "retail_data_docs = retail_data_docsearch.max_marginal_relevance_search(retail_query, k=3)\n",
    "retail_data_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b5d913c-6a9c-43fc-9eef-1e978cbc5dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '$114.99 and 6'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": retail_data_docs, \"question\": retail_query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08651a-993e-4b69-99a8-5b25451a50a9",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this notebook, you should clean up your AWS resources to avoid any unwanted charges. This includes deleting the SageMaker endpoint. [add cleanup steps]"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "noteable": {
   "last_transaction_id": "07d0066e-b9dd-4343-87d9-2e007e4a83d1"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "0070049a-0f1b-5328-927c-3f92cabf9583",
    "openai_ephemeral_user_id": "6f4539cc-18e6-5a48-a00c-a8fe1e1c005a"
   }
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
