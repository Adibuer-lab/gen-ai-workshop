{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6860380-ddd6-4fb7-9fc8-a765049cde01",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": []
   },
   "source": [
    "# Workshop: Text2Text Generation with SageMaker\n",
    "\n",
    "Welcome to this workshop on Text2Text Generation with SageMaker. In this workshop, we will be using a pre-trained model deployed on a SageMaker endpoint to perform text-to-text generation tasks.\n",
    "The workshop is divided into several sections:\n",
    "\n",
    "1. **Setting up the environment:** In this section, we will import necessary libraries and define some helper functions.\n",
    "2. **Querying the endpoint:** We will define some example input texts and use them to query the SageMaker endpoint.\n",
    "3. **Advanced features:** We will explore some advanced features of the model, such as controlling the length of the generated text and the number of output sequences returned.\n",
    "3. **Prompt Engineering:** We will explore some prompt engineering tactics\n",
    "4. **RAG with FAISS:** We will use the langchain library to create a question answering chain and perform similarity searches on a set of documents.\n",
    "5. **Cleaning up:** Finally, we will shut down the SageMaker endpoint to avoid incurring unnecessary costs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c8f25-e6b4-4e23-ab26-130b14b961ad",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": []
   },
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "In this section, we will import the necessary libraries and define some helper functions that we will use throughout the workshop.\n",
    "\n",
    "We will be using the `json` and `boto3` libraries. The `json` library provides functions for working with JSON data, and the `boto3` library allows us to interact with AWS services, including SageMaker.\n",
    "\n",
    "Let's start by importing these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d97202-0ccb-4c46-bc4c-2dd59522fbc5",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fa874-75f8-409e-a3bd-9ca52046f8fe",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define some example input texts. These are the texts that we will use to query the SageMaker endpoint. The model will take these texts as input and return the output of the accomplished task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5de5533-057c-42c7-9bb1-b82fcc597d0d",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = \"Translate the following text to German: My name is Arthur\"\n",
    "text2 = \"A step by step recipe to make bolognese pasta:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed01cc6-5788-4c56-b922-510b79954172",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's define the endpoint that you have created. We will use this endpoint to query the model and get the generated text. We will also define some formatting variables for better output visualization.\n",
    "\n",
    "The `endpoint_name` variable should be set to the name of the SageMaker endpoint that you have created. The `newline`, `bold`, and `unbold` variables are used to format the output text for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989560f3-0e09-46c0-91a8-dda423fe14f1",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "endpoint_name = 'jumpstart-dft-hf-text2text-flan-t5-xl'\n",
    "embedding_endpoint_name = 'jumpstart-dft-hf-textembedding-gpt-j-6b-fp16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383d993-92d1-453e-9c65-c5844f1e8ca1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define a function to query the endpoint. This function will take the encoded text as input and return the response from the endpoint.\n",
    "\n",
    "The `query_endpoint` function uses the `boto3` library to create a SageMaker runtime client. It then uses this client to invoke the SageMaker endpoint with the encoded text as input. The function returns the response from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7899e9d-2717-496c-bba5-843c11305be0",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint(encoded_text):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-text', Body=encoded_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12271244-7b57-4e39-800a-07f033c7c810",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will also define a function to parse the response from the endpoint. This function will extract the generated text from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6717efa9-3cc6-48fe-8c71-c0efc1df3359",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e931f-fb93-4de4-b7f9-be4a8ccaf4e8",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's use these functions to query the endpoint with our example texts and print the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "317c492f-114f-4af6-82b2-c8da833739d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    query_response = query_endpoint(prompt.encode('utf-8'))\n",
    "    generated_text = parse_response(query_response)\n",
    "    print (f\"Inference:{newline}\"\n",
    "            f\"input text: {text}{newline}\"\n",
    "            f\"generated text: {bold}{generated_text}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ddaeefc-95e6-4a41-943f-d54c942c2c47",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate the following text to German: My name is Arthur\n",
      "generated text: \u001b[1mIch bin Arthur.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: A step by step recipe to make bolognese pasta:\n",
      "generated text: \u001b[1mIn a large saucepan, combine the ground beef, onion, garlic, tomato paste, tomato\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in [text1, text2]:\n",
    "    get_completion(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2ad49-201f-4334-a27b-e62b9ba9f2c2",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": []
   },
   "source": [
    "### Advanced Features\n",
    "\n",
    "The model we are using supports many advanced parameters that can be used to control the text generation process. These parameters include:\n",
    "\n",
    "- **max_length:** This parameter controls the maximum length of the generated text. The model will generate text until the output length (which includes the input context length) reaches `max_length`.\n",
    "- **num_return_sequences:** This parameter controls the number of output sequences returned by the model.\n",
    "- **num_beams:** This parameter controls the number of beams used in the greedy search during text generation.\n",
    "- **no_repeat_ngram_size:** This parameter ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence.\n",
    "- **temperature:** This parameter controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words.\n",
    "- **early_stopping:** If set to True, text generation is finished when all beam hypotheses reach the end of sentence token.\n",
    "- **do_sample:** If set to True, the model will sample the next word as per the likelihood.\n",
    "- **top_k:** In each step of text generation, the model will sample from only the `top_k` most likely words.\n",
    "- **top_p:** In each step of text generation, the model will sample from the smallest possible set of words with cumulative probability `top_p`.\n",
    "- **seed:** This parameter can be used to fix the randomized state for reproducibility.\n",
    "\n",
    "We can specify any subset of these parameters when invoking the endpoint. In the next section, we will show an example of how to invoke the endpoint with these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faacecfc-a9d6-44c2-ae23-89888ddb54c4",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":\"Tell me the steps to make a pizza:\", \"max_length\":50, \"num_return_sequences\":3, \"top_k\":50, \"top_p\":0.95, \"do_sample\":True, \"no_repeat_ngram_size\":20}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de10c2-ff8a-4d04-b51e-c57ebb4237b6",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now define a function to query the endpoint with a JSON payload. This function will take the encoded JSON as input and return the response from the endpoint.\n",
    "\n",
    "The `query_endpoint_with_json_payload` function is similar to the `query_endpoint` function we defined earlier. The difference is that this function takes a JSON payload as input instead of a text. This allows us to pass the advanced parameters to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e974115-528c-4520-80bd-df98a531efde",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab43614-a39c-42f8-8301-a260b61a60df",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will also define a function to parse the response from the endpoint when multiple texts are returned. This function will extract the generated texts from the response.\n",
    "\n",
    "The `parse_response_multiple_texts` function is similar to the `parse_response` function we defined earlier. The difference is that this function extracts the 'generated_texts' field from the JSON instead of the 'generated_text' field. This is because when we request multiple texts from the endpoint, the response contains a 'generated_texts' field with a list of generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbf4a45b-c164-463b-b372-b3fb0b0f0594",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions['generated_texts']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7420d-affe-4f44-b67d-074e459b26eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's use these functions to query the endpoint with our JSON payload and print the generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7354401b-c3b1-40b0-ac61-5d4068d965e1",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To make a pizza, first gather your ingredients. Next, spread your pizza sauce on your pizza crust. Next, place your toppings on your pizza crust. Finally, place your pizza in the oven.', 'Gather the ingredients. Place the dough on a floured surface and knead it into a ball. Flatten the ball into a circle and place it on a greased baking sheet. Bake the pizza', 'Spread pizza sauce on the bottom of a large pizza pan. Spread the pizza sauce over the pizza dough. Top the pizza with pepperoni, olives, and other desired toppings. Bake the pizza at 450 degrees F for about']\n"
     ]
    }
   ],
   "source": [
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38b2a6f6-a946-46e9-851f-d35c05797e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_query_endpoint(query):\n",
    "    payload = {\n",
    "        \"text_inputs\": query,\n",
    "        \"max_length\": 5000,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=json.dumps(payload).encode('utf-8'))\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_completion(query):\n",
    "    return parse_response_multiple_texts(\n",
    "        my_query_endpoint(query)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0bf0b-157d-4479-b021-12671547defa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c96d9-6293-4e03-97af-5dd6d0bcce64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prompting Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a6b71-88f9-43ac-bb5e-919772b0fde0",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Write clear and specific instructions.\n",
    "2. Give the model time to “think”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c971114-cf3d-4bec-9b91-bb20a2a4901f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tactics for 'Write clear and specific instructions'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48caea10-ca2a-4cf5-a831-65e979a05763",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d1756c6-6138-446e-868c-8a88921cbf7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CodeWhisperer provides a natural language based, intelligent suggestion engine that uses language models trained on billions of lines of code.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"CodeWhisperer is an AI-powered coding companion designed to assist developers in \\\n",
    "real-time within their Integrated Development Environment (IDE). \\\n",
    "It provides single-line or full-function code suggestions based on natural language comments, \\\n",
    "such as specific tasks or instructions. The suggestions are generated from large language models \\\n",
    "trained on billions of lines of code, including Amazon and open-source code. \\\n",
    "Developers can quickly accept, review, or continue writing their code, \\\n",
    "with the ability to edit suggestions to ensure accuracy. CodeWhisperer also offers \\\n",
    "specialized training for AWS APIs and helps in improving application security by detecting vulnerabilities. \\\n",
    "It supports multiple programming languages and can be used across various IDEs. \\\n",
    "The service also emphasizes responsible AI use, including bias filtering and \\\n",
    "tracking of suggestions that might resemble open-source training data.\"\n",
    "\n",
    "delimiter_prompt = f\"Summarize the text delimited by triple backticks into a single sentence:\\n```{text}```\"\n",
    "\n",
    "get_completion(delimiter_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05989d1-0c39-4ea9-a677-6a19bfd7d9e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 2: Ask for a structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "890e2768-c0a9-42ff-ad92-3d059c404a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CodeWhisperer is an AI-powered coding companion designed to assist developers in real-time within their Integrated Development Environment (IDE) with suggestions based on natural language comments such as specific tasks or instructions, which are generated from large language models trained on billions of lines of code, including Amazon and open-source code. The suggestions are generated from large language models trained on billions of lines of code, including Amazon and open-source code.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"Summarise the key features of CodeWhisperer outlined in the text delimited by triple backticks\\\n",
    "into a single sentence. And output them in JSON form with key being a name of the feature and value \\\n",
    "being the description.For example,'feature1':'description1'\\\n",
    "```{text}```\"\n",
    "\n",
    "get_completion(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d636b-540e-4e25-9a08-836bccc3911b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbeb259e-a335-4850-b607-264b9e8cb5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Step 1 - Choose your IDE Step 2 - Install or update your IDE (if applicable) Step 3 - Install or update the AWS Toolkit (if applicable) Step 4 - Choose your authentication method']\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Before you use CodeWhisperer for the first time, you do the following: \\\n",
    "Choose your IDE. Install or update your IDE (if applicable). \\\n",
    "Install or update the AWS Toolkit (if applicable). Choose your authentication method.\\\n",
    "Set up your Builder ID, IAM Identity Center, or IAM credentials.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50fcf18f-613f-4b91-ad42-e1eec68544dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No steps provided']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"CodeWhisperer is an AI-powered coding companion designed to assist developers in \\\n",
    "real-time within their Integrated Development Environment (IDE). \\\n",
    "It provides single-line or full-function code suggestions based on natural language comments, \\\n",
    "such as specific tasks or instructions. The suggestions are generated from large language models \\\n",
    "trained on billions of lines of code, including Amazon and open-source code. \\\n",
    "Developers can quickly accept, review, or continue writing their code, \\\n",
    "with the ability to edit suggestions to ensure accuracy. CodeWhisperer also offers \\\n",
    "specialized training for AWS APIs and helps in improving application security by detecting vulnerabilities. \\\n",
    "It supports multiple programming languages and can be used across various IDEs. \\\n",
    "The service also emphasizes responsible AI use, including bias filtering and \\\n",
    "tracking of suggestions that might resemble open-source training data.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574d8af-7600-4613-8e81-956c49457677",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 4: \"Few-shot\" prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44263af0-a89a-4e6a-a8be-86050dee4cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grandparent>: You can  tell a liar by the way he looks at things  you can  guess the best way a writer would tell a story  you can  predict how a story will end,   if you know the outcome, you can  prepare yourself to face it; you can  change your mind about your plans and your ideas  if the first one doesn  work out.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2f3bc-771f-4945-a40f-aba915ffa242",
   "metadata": {},
   "source": [
    "### Tactics for 'Give the model time to “think”.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1bb38e-ba91-4ea3-bbe9-9ce06ff08322",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38f2cd66-1ac2-4c9b-b6bd-0fdd29867bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completion for prompt 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"CodeWhisperer est une aide d'apprentissage de la programmation en application de la CI créée pour aider les développeurs dans l'écriture de code dans un environnement intégré de développement (IDE). Il fournit des suggestions de programmes de sexe à à droite ou de full-function dans une linie de comment, comme des tâches spécifiques ou des instructions. Il prend également en compte des évaluations de normes de modèle de langues sur des milliards de péchés de code, dont des possibilités de disponibilité et de disponibilité non-privée. Les développeurs peuvent rapidement accepter, révuer ou poursuivre l'écriture de leurs programmes, y compris pour les suggestions, avec la possibilité d'éditer les conseils afin de s’assurer d'une précision. CodeWhisperer offre également une formation spécialisée pour les APIs AWS, et contribue à renforcer la sécurité des applications en détectant les vulnérabilités. Le service appuie différents langues de programme et peut être utilisé dans diverses environnements de développement.\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"CodeWhisperer is an AI-powered coding companion designed to assist developers in \\\n",
    "real-time within their Integrated Development Environment (IDE). \\\n",
    "It provides single-line or full-function code suggestions based on natural language comments, \\\n",
    "such as specific tasks or instructions. The suggestions are generated from large language models \\\n",
    "trained on billions of lines of code, including Amazon and open-source code. \\\n",
    "Developers can quickly accept, review, or continue writing their code, \\\n",
    "with the ability to edit suggestions to ensure accuracy. CodeWhisperer also offers \\\n",
    "specialized training for AWS APIs and helps in improving application security by detecting vulnerabilities. \\\n",
    "It supports multiple programming languages and can be used across various IDEs. \\\n",
    "The service also emphasizes responsible AI use, including bias filtering and \\\n",
    "tracking of suggestions that might resemble open-source training data.\"\n",
    "# example 1\n",
    "prompt = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "print(\"\\nCompletion for prompt 1:\")\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50907102-e86f-41a7-ae17-220ead42b23b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72449e29-8fbb-4c80-84dd-f71997141973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c3bcd-f5bd-4eb1-90c4-d726fa641a67",
   "metadata": {},
   "source": [
    "#### Note that the student's solution is actually not correct.\n",
    "#### We can fix this by instructing the model to work out its own solution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69939c78-3453-42b3-a05c-2264ed2cd19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" X = cost of land / square foot if the installation is 1000 square feet. Then it's 100x if the land is 1000 square feet. Then 250x if the solar panels are 1000 square feet and 10x if the maintenance is 100,000 square feet. Total cost if the land and panels are 1000 square feet: 100x + 250x + 100,000 + 100x = 450x + 100,000.  Correct.\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following,\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format\n",
    "Question\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated?\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution. Analyse it step by step when explaining your reasoning:\n",
    "\"\"\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ff671-3b88-4611-859e-c2fb2f2cc077",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 4: Model Limitations: Hallucinations\n",
    "- Boie is a real company, the product name is not real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16db12bc-86b7-4128-9abd-7d53eb708b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For each line of code, Codewhisperer suggests a better solution or a possible refactoring solution. You can also enter a new line to help Codewhisperer understand the problem.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "How does codewhisperer generate suggestions?\n",
    "\"\"\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2f5b1-362a-4bab-9436-1036be407429",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 5: RAG with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b683e3-0ede-4205-a63d-57b63850a270",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Before we proceed to the next steps, let's ensure that we have the necessary libraries installed. We will need the `langchain` library for the following steps. If it's not already installed, we can install it using pip.\n",
    "\n",
    "The `langchain` library is a Python library that provides utilities for working with large language models. It includes utilities for creating prompts, querying endpoints, parsing responses, and more. We will use this library in the following steps to interact with our SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33298503-afb7-4170-8867-e624c335da6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Get:2 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:3 http://security.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:4 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [253 kB]\n",
      "Fetched 345 kB in 0s (1269 kB/s)33m[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libmagic-dev is already the newest version (1:5.39-3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt-get install libmagic-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9c0efe3-428d-4e12-a38e-b3671ac24736",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade python-magic unstructured langchain faiss-cpu pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63738465-b0bd-48f3-b436-66c5e580b1eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's import some necessary modules from the `langchain` library.\n",
    "\n",
    "- `PromptTemplate`: This class is used to create a template for the prompts that we will pass to the language model. \n",
    "- `SagemakerEndpoint`: This class is used to interact with the SageMaker endpoint.\n",
    "- `LLMContentHandler`: This class is used to handle the content that we send to and receive from the language model.\n",
    "- `load_qa_chain`: This function is used to load a question-answering chain. A chain is a sequence of transformations applied to the input to generate an answer.\n",
    "- `Document`: This class is used to create documents that the language model can use to find the answer to a question.\n",
    "- `EmbeddingsContentHandler`: This class is used to handle the content that we send to and receive from the embedding model.\n",
    "- `SagemakerEndpointEmbeddings`: This class is used to interact with the SageMaker embeddings enpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ba22d63-2d2a-462d-a39e-88b5d2a6d1f6",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain, LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dffc3-227c-4269-94a1-b836d8e7ffd1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now create a content handler for the language model to transform input to a format that the SageMaker endpoint expects and output to a form that the language model class expects. We will also define some parameters for the model.\n",
    "\n",
    "The `ContentHandler` class is a subclass of the `LLMContentHandler` class. It defines two methods:\n",
    "\n",
    "- `transform_input`: This method takes a prompt and a dictionary of model parameters as input, and returns the input in a format that the SageMaker endpoint expects. In this case, it converts the input to a JSON string and encodes it to bytes.\n",
    "- `transform_output`: This method takes the output from the SageMaker endpoint and returns it in a form that the language model class expects. In this case, it decodes the output from bytes to a string, parses the JSON, and returns the 'generated_texts' field.\n",
    "\n",
    "The `parameters` dictionary defines the parameters that we will use when querying the language model. These parameters control the behavior of the language model, such as the maximum length of the generated text, the number of sequences to return, and the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d01c2de-8ab3-4c2c-bd6e-a69664744287",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 5000,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.01,\n",
    "}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json['generated_texts'][0]\n",
    "    \n",
    "\n",
    "\n",
    "llm_content_handler = ContentHandler()\n",
    "sm_llm=SagemakerEndpoint(\n",
    "            endpoint_name=endpoint_name,\n",
    "            region_name=\"eu-central-1\",\n",
    "            model_kwargs=parameters,\n",
    "            content_handler=llm_content_handler,\n",
    "        )\n",
    "creative_llm=SagemakerEndpoint(\n",
    "            endpoint_name=endpoint_name,\n",
    "            region_name=\"eu-central-1\",\n",
    "            model_kwargs={\n",
    "                \"max_length\": 5000,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"top_k\": 250,\n",
    "                \"top_p\": 0.95,\n",
    "                \"do_sample\": False,\n",
    "                \"temperature\": 2.5\n",
    "            },\n",
    "            content_handler=llm_content_handler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f130c-b368-4562-b863-6d1f5062cd3d",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define a prompt template and load a chain. \n",
    "\n",
    "The prompt template is used to format the input to the language model. It accepts a set of parameters from the user that can be used to generate a prompt for a language model. \n",
    "\n",
    "The question answering chain is a sequence of transformations applied to the input to generate an answer.\n",
    "\n",
    "The `PromptTemplate` class takes a template string and a list of input variables as arguments. The template string is a string that contains placeholders for the input variables. The placeholders are enclosed in curly braces `{}` and correspond to the names of the input variables. When we use the prompt template, we will replace the placeholders with the actual values of the input variables.\n",
    "\n",
    "The `chain` function loads a chain. A chain is a sequence of transformations applied to the input to generate an answer. Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. In this case, the chain includes the language model and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc35a925-898d-46f3-bab7-fefd896983fc",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "            template=\"Use the following pieces of context to answer the question at the end.\\n{context}\\nQuestion: {question}\\nAnswer:\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "chain = load_qa_chain(\n",
    "        llm=sm_llm,\n",
    "        prompt=prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afa2f4-e7c3-4eb7-8095-6c8ebd9e5ebe",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's test our question answering chain with a sample question and some context. The context is a list of documents that the model can use to find the answer to the question.\n",
    "\n",
    "The `chain` function takes a dictionary as input and returns the output of the chain. The input dictionary must contain the 'input_documents' and 'question' keys. The 'input_documents' key corresponds to a list of documents that the model can use to find the answer to the question. The 'question' key corresponds to the question that we want to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a267d2db-d36a-4d70-a977-d4c26e189db2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'SageMaker'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "input_documents = [Document(page_content=\"\")]\n",
    "\n",
    "chain({\"input_documents\": input_documents, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06cb55-e53c-4a89-85a3-ff8128b4a6a1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will create a content handler for embeddings to transform a format that the SageMaker endpoint expects and output to a form that the embeddings class expects.\n",
    "\n",
    "The `SagemakerEndpointEmbeddingsJumpStart` class is a subclass of the `SagemakerEndpointEmbeddings` class. It defines the `embed_documents` method, which computes document embeddings using a SageMaker Inference Endpoint. The method takes a list of texts and a chunk size as input, and returns a list of embeddings.\n",
    "\n",
    "The `ContentHandler` class is a subclass of the `EmbeddingsContentHandler` class. It defines two methods:\n",
    "\n",
    "- `transform_input`: This method takes a prompt and a dictionary of model parameters as input, and returns the input in a format that the SageMaker endpoint expects. In this case, it converts the input to a JSON string and encodes it to bytes.\n",
    "- `transform_output`: This method takes the output from the SageMaker endpoint and returns it in a form that the embeddings class expects. In this case, it decodes the output from bytes to a string, parses the JSON, and returns the 'embedding' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb0f7dc1-6584-4c9e-a2bb-30e465881cf2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "embeddings_content_handler=ContentHandler()\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=\"eu-central-1\",\n",
    "    content_handler=embeddings_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879915e9-6ef5-41cd-a986-3abf044ec7c8",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now we will load the data we will embed for contextual prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82d8140a-c7f4-44fe-a930-207c54539717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.url import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b8361d0-c950-4bc7-b2c9-a8d16e9515e2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://aws.amazon.com/codewhisperer/faqs/\",\n",
    "    \"https://aws.amazon.com/sagemaker/faqs/\",\n",
    "]\n",
    "headers={\"ssl_verify\":\"False\"}\n",
    "loader = UnstructuredURLLoader(urls=urls,headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7305ce-64a7-493e-a816-930c443b6670",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now install the `faiss-cpu` library\n",
    "\n",
    "`faiss-cpu` provides efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI that allows for efficient similarity search and clustering of dense vectors. So, given a set of vectors(in this case a vector representation of a document i.e. an embedding), we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c7fb7ce-54a3-4118-bfa7-32f79192f222",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cb248-4acb-4b4d-adbd-54e2f4ec9997",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now create an index of our documents using the VectorstoreIndexCreator. This index will allow us to perform efficient similarity searches on our documents.\n",
    "\n",
    "The VectorstoreIndexCreator is a utility that helps us create an index of our documents. It uses the embeddings of the documents to create the index. The embeddings are dense vectors that represent the documents. The index allows us to perform efficient similarity searches on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0f96ef5-4026-49fc-a0a0-4aaa1f2e9395",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader,CSVLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter = text_splitter\n",
    ")\n",
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcac91-176b-4cf3-8f6b-eeb8cd469751",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Let's test our index by querying it with a sample question.\n",
    "\n",
    "The `index.query` function is used to perform a similarity search on the index. It takes a question and a language model as input, and returns the most similar documents in the index. After the relevant documents are retrieved, the LLM can be used to generate a coherent and contextually relevant answer based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "175c4fc3-a0e9-4379-9832-9e5ebfae187a",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all instances supported in SageMaker'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(question=query, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee42e36-d687-45f1-b0d5-befdf5c16f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will now replicate the index.query functionality step by step to illustrate what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a07a4-5044-4639-8351-be6399eabf53",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "we will create a document search object using the FAISS vector store and our documents. This will allow us to perform similarity searches on our documents. Using this we retrieve the top 3 most similar docs to our query.\n",
    "\n",
    "The `FAISS.from_documents` function is used to create a FAISS vector store from our documents. The embeddings of the documents are used to create the vector store. The vector store allows us to perform efficient similarity searches on the documents.\n",
    "\n",
    "The `docsearch.similarity_search` function is used to perform a similarity search on the documents. It takes a query and a number of results to return as input, and returns the most similar documents in the vector store. The query is converted into an embedding and this embedding is then compared with the embeddings of the documents in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a43072c-1102-400b-b5f7-d4a52b8d614f",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q: Which instances can I use with Managed Spot Training?\\n\\nManaged Spot Training can be used with all instances supported in SageMaker.\\n\\nQ: Which Regions are supported with Managed Spot Training?', metadata={'source': 'https://aws.amazon.com/sagemaker/faqs/', 'start_index': 46752}),\n",
       " Document(page_content='Q: When should I use Managed Spot Training?', metadata={'source': 'https://aws.amazon.com/sagemaker/faqs/', 'start_index': 45014}),\n",
       " Document(page_content='Q: Why should I use SageMaker Serverless Inference?', metadata={'source': 'https://aws.amazon.com/sagemaker/faqs/', 'start_index': 55822})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "splitdocuments = text_splitter.split_documents(documents)\n",
    "docsearch = FAISS.from_documents(splitdocuments, embeddings)\n",
    "docs = docsearch.max_marginal_relevance_search(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffb027-7392-4e41-9fcc-e1b87e96e7ea",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Finally, we will use our question-answering chain to answer our query using the documents we found.\n",
    "\n",
    "The `chain` function is used to apply our question-answering chain to our query and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65db642c-0143-4adb-8b61-4f556966bb76",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'all instances supported in SageMaker'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec624fcc-68b6-428d-950d-600ff5336a91",
   "metadata": {},
   "source": [
    "We have looked at two flows so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96249b-a493-475f-b239-1aed1025947c",
   "metadata": {},
   "source": [
    "a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144f57a-8c3f-425e-bb66-91897c60b301",
   "metadata": {},
   "source": [
    "![alt text](flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda94971-ef3f-433c-87f9-d1ccbd2d0174",
   "metadata": {},
   "source": [
    "b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b173f-7078-4e73-abd1-e4d0f5719571",
   "metadata": {},
   "source": [
    "![alt text](RAGflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21152ce5-bf72-4c4f-95a8-c6cd9f8b2ae7",
   "metadata": {},
   "source": [
    "Now let's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b9d6e61-3c55-4f6b-aa4b-a11ffae97bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://mysagebucket-4590283737/RAGFiles/retail_items.csv to rag_data/retail_items.csv\n"
     ]
    }
   ],
   "source": [
    "retail_data = \"s3://mysagebucket-4590283737/RAGFiles/\"\n",
    "!aws s3 cp --recursive $retail_data rag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65ec90a7-0f46-4400-8fca-7affdcde4753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1322/4038575674.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_df['description'] = df.apply(lambda row: f\"{row['name']} is a {row['style']} in the {row['category']} category. Description: {row['description']} with a Price of ${row['price']} and Current stock is {row['current_stock']}.\", axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sans Pareil Scarf is a scarf in the apparel ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chef Knife is a kitchen in the housewares cate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gainsboro Jacket is a jacket in the apparel ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High Definition Speakers is a speaker in the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spiffy Sandals is a sandals in the footwear ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  Sans Pareil Scarf is a scarf in the apparel ca...\n",
       "1  Chef Knife is a kitchen in the housewares cate...\n",
       "2  Gainsboro Jacket is a jacket in the apparel ca...\n",
       "3  High Definition Speakers is a speaker in the e...\n",
       "4  Spiffy Sandals is a sandals in the footwear ca..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('rag_data/retail_items.csv')\n",
    "\n",
    "processed_df=df[['description']]\n",
    "processed_df['description'] = df.apply(lambda row: f\"{row['name']} is a {row['style']} in the {row['category']} category. Description: {row['description']} with a Price of ${row['price']} and Current stock is {row['current_stock']}.\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "processed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b93fe927-91b3-4789-9f68-01da8f12a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[['description']].to_csv(\"rag_data/processed_retail_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7c71116-304f-44b9-a76e-900342fde412",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_data_loader = CSVLoader(file_path=\"rag_data/processed_retail_data.csv\")\n",
    "retail_data_documents = retail_data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ed0c57e-364b-4feb-af52-bc78f54ba0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "retail_data_index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter=text_splitter,\n",
    ")\n",
    "retail_data_index = retail_data_index_creator.from_loaders([retail_data_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d5c7423-4f27-448e-b336-1eaab9c2c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_query=\"What is the price and stock of Sans Pareil scarf?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cba294f8-e525-4bed-b96d-70bc599d3655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'not enough information'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": input_documents, \"question\": retail_query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c51f25d3-4eae-40ac-8814-25773f3b81a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$114.99 and Current stock is 6'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail_data_index.query(question=retail_query, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623b846-c58f-4b6b-b446-7044a6928252",
   "metadata": {},
   "source": [
    "FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b273f924-2f1f-4c1f-af68-19dd3c112445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir training_data\n",
    "!touch training_data/retail_items.jsonl\n",
    "!touch training_data/train_retail_items.jsonl\n",
    "!touch training_data/valid_retail_items.jsonl\n",
    "!touch training_data/temp-task-data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c3667ab9-20a8-4b60-9724-899bee390fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from uuid import uuid4\n",
    "\n",
    "def csv_to_jsonl(csv_file_path, jsonl_file_path):\n",
    "    with open(csv_file_path, 'r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        data = list(reader)\n",
    "        \n",
    "    jsonl_data = {\n",
    "        \"version\": \"v2.0\",\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    for row in data:\n",
    "        gender_affinity = 'women' if row['gender_affinity']=='F' else 'men' if row['gender_affinity']=='M' else 'no gender bias'\n",
    "        aliases = row['aliases'] if row['aliases'] else 'no aliases'\n",
    "        context = f\"The {row['name']} is a {row['style']} style item in the {row['category']} category. It's described as: {row['description']}. The item is priced at {row['price']} and is currently in stock with {row['current_stock']} units available. You can view the item at {row['url']} and its image at {row['image']}. The item is preferred by {gender_affinity} and its SKU is {row['sk']}. The item is also known as {aliases}. The item's ID is {row['id']}.\"\n",
    "        jsonl_data[\"data\"].append(\n",
    "            {\n",
    "                \"title\": row['name'],\n",
    "                \"paragraphs\": [\n",
    "                    {\n",
    "                        \"qas\": [\n",
    "                            {\n",
    "                                \"question\": f\"What is the name of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['name'],\n",
    "                                        \"answer_start\": context.find(row['name'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the style of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['style'],\n",
    "                                        \"answer_start\": context.find(row['style'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What category does the item belong to?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['category'],\n",
    "                                        \"answer_start\": context.find(row['category'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the description of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['description'],\n",
    "                                        \"answer_start\": context.find(row['description'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the price of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['price'],\n",
    "                                        \"answer_start\": context.find(row['price'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"How many units of the item are available?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['current_stock'],\n",
    "                                        \"answer_start\": context.find(row['current_stock'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the URL of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['url'],\n",
    "                                        \"answer_start\": context.find(row['url'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the URL of the item's image?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['image'],\n",
    "                                        \"answer_start\": context.find(row['image'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"Who is the item preferred by?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": gender_affinity,\n",
    "                                        \"answer_start\": context.find(gender_affinity)\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the SKU of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['sk'],\n",
    "                                        \"answer_start\": context.find(row['sk'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What are the aliases of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": aliases,\n",
    "                                        \"answer_start\": context.find(aliases)\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                            {\n",
    "                                \"question\": f\"What is the ID of the item?\",\n",
    "                                \"id\": str(uuid4()),\n",
    "                                \"answers\": [\n",
    "                                    {\n",
    "                                        \"text\": row['id'],\n",
    "                                        \"answer_start\": context.find(row['id'])\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"is_impossible\": False\n",
    "                            },\n",
    "                        ],\n",
    "                        \"context\": context\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    with open(jsonl_file_path, 'w') as jsonl_file:\n",
    "        jsonl_file.write(json.dumps(jsonl_data) + '\\n')\n",
    "\n",
    "csv_to_jsonl('rag_data/retail_items.csv', 'training_data/retail_items.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ef6cc1ec-12ed-4236-8ccf-7769b852d869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_jsonl(jsonl_file_path, train_file_path, valid_file_path, valid_ratio=0.2):\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file:\n",
    "        data = json.load(jsonl_file)\n",
    "        \n",
    "    train_data = {\n",
    "        \"version\": \"v2.0\",\n",
    "        \"data\": data[\"data\"]\n",
    "    }\n",
    "\n",
    "    valid_data = {\n",
    "        \"version\": \"v2.0\",\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    valid_size = int(len(data[\"data\"]) * valid_ratio)\n",
    "\n",
    "    for i, item in enumerate(data[\"data\"]):\n",
    "        if i < valid_size:\n",
    "            valid_data[\"data\"].append(item)\n",
    "        \n",
    "    with open(train_file_path, 'w') as train_file:\n",
    "        train_file.write(json.dumps(train_data) + '\\n')\n",
    "\n",
    "    with open(valid_file_path, 'w') as valid_file:\n",
    "        valid_file.write(json.dumps(valid_data) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "split_jsonl('training_data/retail_items.jsonl', 'training_data/train_retail_items.jsonl', 'training_data/valid_retail_items.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ab3f623-46be-4d17-9c44-d4105f5e8eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7d6a6e9e-7792-4413-a50e-c1c6583f7b34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1maws_region:\u001b[0m eu-central-1\n",
      "\u001b[1maws_role:\u001b[0m arn:aws:iam::881683121006:role/service-role/AmazonSageMaker-ExecutionRole-20230623T100989\n",
      "\u001b[1moutput_bucket:\u001b[0m sagemaker-eu-central-1-881683121006\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "# Get current region, role, and default bucket\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_role = sagemaker.session.Session().get_caller_identity_arn()\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "# This will be useful for printing\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "print(f\"{bold}aws_region:{unbold} {aws_region}\")\n",
    "print(f\"{bold}aws_role:{unbold} {aws_role}\")\n",
    "print(f\"{bold}output_bucket:{unbold} {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4ae6bb34-c5fb-425c-8476-7ed2457d3b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmodel_id:\u001b[0m huggingface-text2text-flan-t5-xl\n",
      "\u001b[1mtraining_instance_type:\u001b[0m ml.g5.48xlarge\n",
      "\u001b[1minference_instance_type:\u001b[0m ml.g5.2xlarge\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.instance_types import retrieve_default\n",
    "model_id, model_version = dropdown.value, \"*\"\n",
    "# Instance types for training and inference\n",
    "training_instance_type = 'ml.g5.48xlarge'\n",
    "inference_instance_type = retrieve_default(\n",
    "model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "print(f\"{bold}model_id:{unbold} {model_id}\")\n",
    "print(f\"{bold}training_instance_type:{unbold} {training_instance_type}\")\n",
    "print(f\"{bold}inference_instance_type:{unbold} {inference_instance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f443523e-b1d7-4404-81ad-aeb692ef156d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.2.5'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.2.4'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.2.3'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.2.2'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.2.1'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.2.0'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-base' and version '1.1.0'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.6'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.5'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.4'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.3'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.2'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.1'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-large' and version '1.1.0'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.2.5'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.2.4'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.2.3'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.2.2'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.2.1'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.2.0'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-small' and version '1.1.0'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.6'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.5'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.4'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.3'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.2'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.1'.\n",
      "WARNING:sagemaker.jumpstart:Using vulnerable JumpStart model 'huggingface-text2text-flan-t5-xl' and version '1.1.0'.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Select a pre-trained model from the dropdown below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a52ba5fdb854f46b197c5b6567bd0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='FLAN T5 models available for fine-tuning:', index=3, layout=Layout(width='max-content'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.filters import And\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "# Default model choice\n",
    "model_id = \"huggingface-text2text-flan-t5-xl\"\n",
    "# Identify FLAN T5 models that support fine-tuning\n",
    "filter_value = And(\n",
    "\"task == text2text\", \"framework == huggingface\", \"training_supported == true\"\n",
    ")\n",
    "model_list = [m for m in list_jumpstart_models(filter=filter_value) if \"flan-t5\" in m]\n",
    "# Display the model IDs in a dropdown, for user to select\n",
    "dropdown = Dropdown(\n",
    "value=model_id,\n",
    "options=model_list,\n",
    "description=\"FLAN T5 models available for fine-tuning:\",\n",
    "style={\"description_width\": \"initial\"},\n",
    "layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"### Select a pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ebe31e2b-b3c6-478d-8a01-4a9f6dd443af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mimage uri:\u001b[0m 763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      "\u001b[1mmodel uri:\u001b[0m s3://jumpstart-cache-prod-eu-central-1/huggingface-training/train-huggingface-text2text-flan-t5-xl.tar.gz\n",
      "\u001b[1mscript uri:\u001b[0m s3://jumpstart-cache-prod-eu-central-1/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.1.2/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "# Training instance will use this image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "region=aws_region,\n",
    "framework=None,  # automatically inferred from model_id\n",
    "model_id=model_id,\n",
    "model_version=model_version,\n",
    "image_scope=\"training\",\n",
    "instance_type=training_instance_type,\n",
    ")\n",
    "# Pre-trained model\n",
    "train_model_uri = model_uris.retrieve(\n",
    "model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "# Script to execute on the training instance\n",
    "train_script_uri = script_uris.retrieve(\n",
    "model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "print(f\"{bold}image uri:{unbold} {train_image_uri}\")\n",
    "print(f\"{bold}model uri:{unbold} {train_model_uri}\")\n",
    "print(f\"{bold}script uri:{unbold} {train_script_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dd6e42b2-734f-47ad-9410-c31f70c1a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtraining data:\u001b[0m s3://sagemaker-studio-881683121006-yxaepgxvcl/training_flan/training/notebookgen\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "original_data_file = \"training_data/retail_items.jsonl\"\n",
    "\n",
    "local_data_file = \"training_data/temp-task-data.jsonl\"  # any name with .jsonl extension\n",
    "\n",
    "with open(original_data_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(local_data_file, \"w\") as f:\n",
    "    for article in data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            # iterate over questions for a given paragraph\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                example = {\"context\": paragraph[\"context\"], \"question\": qas[\"question\"], \"answer\": qas[\"answers\"][0][\"text\"]}\n",
    "                json.dump(example, f)\n",
    "                f.write(\"\\n\")\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)\n",
    "\n",
    "\n",
    "train_data_location = \"s3://sagemaker-studio-881683121006-yxaepgxvcl/training_flan/training/notebookgen\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"{bold}training data:{unbold} {train_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8f958401-d925-4c30-be4e-834faa1114d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': '3', 'max_steps': '-1', 'seed': '42', 'batch_size': '64', 'learning_rate': '0.0001', 'lr_scheduler_type': 'constant_with_warmup', 'warmup_ratio': '0.0', 'warmup_steps': '0', 'validation_split_ratio': '0.05', 'train_data_split_seed': '0', 'max_train_samples': '-1', 'max_eval_samples': '-1', 'max_input_length': '-1', 'max_output_length': '128', 'pad_to_max_length': 'True', 'gradient_accumulation_steps': '1', 'weight_decay': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'load_best_model_at_end': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'label_smoothing_factor': '0', 'logging_strategy': 'steps', 'logging_first_step': 'False', 'logging_steps': '500', 'logging_nan_inf_filter': 'True', 'save_strategy': 'epoch', 'save_steps': '500', 'save_total_limit': '2', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'evalaution_strategy': 'epoch', 'eval_steps': '500', 'eval_accumulation_steps': 'None', 'gradient_checkpointing': 'True', 'auto_find_batch_size': 'False', 'preprocessing_num_workers': 'None', 'peft_type': 'none'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# We will override some default hyperparameters with custom values\n",
    "hyperparameters[\"epochs\"] = \"3\"\n",
    "# TODO\n",
    "# hyperparameters[\"max_input_length\"] = \"300\"  # data inputs will be truncated at this length\n",
    "# hyperparameters[\"max_output_length\"] = \"40\"  # data outputs will be truncated at this length\n",
    "# hyperparameters[\"generation_max_length\"] = \"40\"  # max length of generated output\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "796472f8-7f62-4f00-bdd8-e1525dba7585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: js-demo-flan-t5-xl-3-2023-08-07-05-09-06-656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mjob name:\u001b[0m js-demo-flan-t5-xl-3-2023-08-07-05-09-06-656\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g5.48xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 34\u001b[0m\n\u001b[1;32m     16\u001b[0m sm_estimator \u001b[38;5;241m=\u001b[39m Estimator(\n\u001b[1;32m     17\u001b[0m role\u001b[38;5;241m=\u001b[39maws_role,\n\u001b[1;32m     18\u001b[0m image_uri\u001b[38;5;241m=\u001b[39mtrain_image_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m metric_definitions\u001b[38;5;241m=\u001b[39mtraining_metric_definitions,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Launch a SageMaker training job over data located in the given S3 path\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Training jobs can take hours, it is recommended to set wait=False,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# and monitor job status through SageMaker console\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43msm_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_job_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1289\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1288\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2255\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m \n\u001b[1;32m   2232\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;124;03m    all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m-> 2255\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:892\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    889\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m--> 892\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5494\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5478\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5479\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5482\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5483\u001b[0m ):\n\u001b[1;32m   5484\u001b[0m     \u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5485\u001b[0m \n\u001b[1;32m   5486\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5492\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5493\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:890\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    888\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m    889\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 890\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g5.48xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota."
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "output_location=\"s3://sagemaker-studio-881683121006-yxaepgxvcl/finetuned/\"\n",
    "model_name = \"-\".join(model_id.split(\"-\")[2:])  # get the most informative part of ID\n",
    "training_job_name = name_from_base(f\"js-demo-{model_name}-{hyperparameters['epochs']}\")\n",
    "print(f\"{bold}job name:{unbold} {training_job_name}\")\n",
    "\n",
    "training_metric_definitions = [\n",
    "{\"Name\": \"val_loss\", \"Regex\": \"'eval_loss': ([0-9\\\\.]+)\"},\n",
    "{\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9\\\\.]+)\"},\n",
    "{\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "sm_estimator = Estimator(\n",
    "role=aws_role,\n",
    "image_uri=train_image_uri,\n",
    "model_uri=train_model_uri,\n",
    "source_dir=train_script_uri,\n",
    "entry_point=\"transfer_learning.py\",\n",
    "instance_count=1,\n",
    "instance_type=training_instance_type,\n",
    "volume_size=300,\n",
    "max_run=360000,\n",
    "hyperparameters=hyperparameters,\n",
    "output_path=output_location,\n",
    "metric_definitions=training_metric_definitions,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker training job over data located in the given S3 path\n",
    "# Training jobs can take hours, it is recommended to set wait=False,\n",
    "# and monitor job status through SageMaker console\n",
    "sm_estimator.fit({\"training\": train_data_location,}, job_name=training_job_name, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1e9dcf58-b9a9-44d2-9a0c-60a0a43aa803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.analytics:Warning: No metrics called val_loss found\n",
      "WARNING:sagemaker.analytics:Warning: No metrics called train_loss found\n",
      "WARNING:sagemaker.analytics:Warning: No metrics called epoch found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# This can be called while the job is still running\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7bec9c57-9b6a-48ec-9a2e-5cee6c55ff90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mimage URI:\u001b[0m\n",
      " 763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      "\u001b[1mmodel URI:\u001b[0m\n",
      " s3://sagemaker-studio-881683121006-yxaepgxvcl/finetuned/js-demo-flan-t5-xl-3-2023-08-06-22-54-21-407/output/model.tar.gz\n",
      "Deploying an endpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!\n",
      "Deployed an endpoint jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Retrieve the inference docker image URI. This is the base HuggingFace container image\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "fine_tuned_name = name_from_base(f\"jumpstart-demo-fine-tuned-{model_id}\")\n",
    "fine_tuned_model_uri = f\"{output_location}{training_job_name}/output/model.tar.gz\"\n",
    "\n",
    "# Create the SageMaker model instance of the fine-tuned model\n",
    "fine_tuned_model = Model(\n",
    "image_uri=deploy_image_uri,\n",
    "model_data=fine_tuned_model_uri,\n",
    "role=aws_role,\n",
    "predictor_cls=Predictor,\n",
    "name=fine_tuned_name,\n",
    ")\n",
    "\n",
    "print(f\"{bold}image URI:{unbold}{newline} {deploy_image_uri}\")\n",
    "print(f\"{bold}model URI:{unbold}{newline} {fine_tuned_model_uri}\")\n",
    "print(\"Deploying an endpoint ...\")\n",
    "\n",
    "# Deploy the fine-tuned model.\n",
    "fine_tuned_predictor = fine_tuned_model.deploy(\n",
    "initial_instance_count=1,\n",
    "instance_type=inference_instance_type,\n",
    "predictor_cls=Predictor,\n",
    "endpoint_name=fine_tuned_name,\n",
    ")\n",
    "print(f\"{newline}Deployed an endpoint {fine_tuned_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7655eab0-3771-4cd0-91f0-217093c20331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_llm=SagemakerEndpoint(\n",
    "            endpoint_name=fine_tuned_name,\n",
    "            region_name=\"eu-central-1\",\n",
    "            model_kwargs=parameters,\n",
    "            content_handler=llm_content_handler,\n",
    "        )\n",
    "prompt=PromptTemplate(\n",
    "            template=\"Use the following pieces of context to answer the question at the end.\\n{context}\\nQuestion: {question}\\nAnswer:\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "fn_chain = load_qa_chain(\n",
    "        llm=fn_llm,\n",
    "        prompt=prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8449cf2b-5495-4113-81b1-f1e389aca8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference endpoint: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"/opt/ml/model does not appear to have a file named config.json. Checkout \\u0027https://huggingface.co//opt/ml/model/None\\u0027 for available files.\"\n}\n\". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246 in account 881683121006 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py:237\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_endpoint_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"/opt/ml/model does not appear to have a file named config.json. Checkout \\u0027https://huggingface.co//opt/ml/model/None\\u0027 for available files.\"\n}\n\". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246 in account 881683121006 for more information.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the price and stock of Sans Pareil scarf?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m input_documents \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mfn_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:258\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    259\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    260\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    261\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    262\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:252\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    246\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    247\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    248\u001b[0m     inputs,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 252\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:106\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    105\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 106\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:165\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:258\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    259\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    260\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    261\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    262\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:252\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    246\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    247\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    248\u001b[0m     inputs,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 252\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:455\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    449\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    453\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    454\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:586\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    581\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    582\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m), [prompt], invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    583\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m callback_manager, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(callback_managers, prompts)\n\u001b[1;32m    585\u001b[0m     ]\n\u001b[0;32m--> 586\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:492\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    491\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    493\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:479\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    471\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    476\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 479\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    483\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    488\u001b[0m         )\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:965\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    964\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 965\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    968\u001b[0m     )\n\u001b[1;32m    969\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py:245\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39minvoke_endpoint(\n\u001b[1;32m    238\u001b[0m         EndpointName\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name,\n\u001b[1;32m    239\u001b[0m         Body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_endpoint_kwargs,\n\u001b[1;32m    243\u001b[0m     )\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_handler\u001b[38;5;241m.\u001b[39mtransform_output(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# This is a bit hacky, but I can't figure out a better way to enforce\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# stop tokens when making calls to the sagemaker endpoint.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference endpoint: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"/opt/ml/model does not appear to have a file named config.json. Checkout \\u0027https://huggingface.co//opt/ml/model/None\\u0027 for available files.\"\n}\n\". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-demo-fine-tuned-huggingface-t-2023-08-07-04-51-18-246 in account 881683121006 for more information."
     ]
    }
   ],
   "source": [
    "query = \"What is the price and stock of Sans Pareil scarf?\"\n",
    "\n",
    "input_documents = [Document(page_content=\"\")]\n",
    "\n",
    "fn_chain({\"input_documents\": input_documents, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08651a-993e-4b69-99a8-5b25451a50a9",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this notebook, you should clean up your AWS resources to avoid any unwanted charges. This includes deleting the SageMaker endpoint. [add cleanup steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066a962-4da4-44e5-88fd-463fe253056c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "noteable": {
   "last_transaction_id": "07d0066e-b9dd-4343-87d9-2e007e4a83d1"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "0070049a-0f1b-5328-927c-3f92cabf9583",
    "openai_ephemeral_user_id": "6f4539cc-18e6-5a48-a00c-a8fe1e1c005a"
   }
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
