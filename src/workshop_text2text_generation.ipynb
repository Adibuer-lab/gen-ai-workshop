{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6860380-ddd6-4fb7-9fc8-a765049cde01",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Workshop: Text2Text Generation with SageMaker\n",
    "\n",
    "Welcome to this workshop on Text2Text Generation with SageMaker. In this workshop, we will be using a pre-trained model deployed on a SageMaker endpoint to perform text-to-text generation tasks.\n",
    "The workshop is divided into several sections:\n",
    "\n",
    "1. **Setting up the environment:** In this section, we will import necessary libraries and define some helper functions.\n",
    "2. **Querying the endpoint:** We will define some example input texts and use them to query the SageMaker endpoint.\n",
    "3. **Advanced features:** We will explore some advanced features of the model, such as controlling the length of the generated text and the number of output sequences returned.\n",
    "4. **Using the langchain library:** We will use the langchain library to create a question answering chain and perform similarity searches on a set of documents.\n",
    "5. **Cleaning up:** Finally, we will shut down the SageMaker endpoint to avoid incurring unnecessary costs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c8f25-e6b4-4e23-ab26-130b14b961ad",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "In this section, we will import the necessary libraries and define some helper functions that we will use throughout the workshop.\n",
    "\n",
    "We will be using the `json` and `boto3` libraries. The `json` library provides functions for working with JSON data, and the `boto3` library allows us to interact with AWS services, including SageMaker.\n",
    "\n",
    "Let's start by importing these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d97202-0ccb-4c46-bc4c-2dd59522fbc5",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fa874-75f8-409e-a3bd-9ca52046f8fe",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define some example input texts. These are the texts that we will use to query the SageMaker endpoint. The model will take these texts as input and return the output of the accomplished task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5de5533-057c-42c7-9bb1-b82fcc597d0d",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "text1 = \"Translate to German:  My name is Arthur\"\n",
    "text2 = \"A step by step recipe to make bolognese pasta:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed01cc6-5788-4c56-b922-510b79954172",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's define the endpoint that you have created. We will use this endpoint to query the model and get the generated text. We will also define some formatting variables for better output visualization.\n",
    "\n",
    "The `endpoint_name` variable should be set to the name of the SageMaker endpoint that you have created. The `newline`, `bold`, and `unbold` variables are used to format the output text for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989560f3-0e09-46c0-91a8-dda423fe14f1",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "endpoint_name = 'jumpstart-dft-hf-text2text-flan-t5-xl'\n",
    "embedding_endpoint_name = 'jumpstart-dft-hf-textembedding-gpt-j-6b-fp16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383d993-92d1-453e-9c65-c5844f1e8ca1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define a function to query the endpoint. This function will take the encoded text as input and return the response from the endpoint.\n",
    "\n",
    "The `query_endpoint` function uses the `boto3` library to create a SageMaker runtime client. It then uses this client to invoke the SageMaker endpoint with the encoded text as input. The function returns the response from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7899e9d-2717-496c-bba5-843c11305be0",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def query_endpoint(encoded_text):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-text', Body=encoded_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12271244-7b57-4e39-800a-07f033c7c810",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will also define a function to parse the response from the endpoint. This function will extract the generated text from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6717efa9-3cc6-48fe-8c71-c0efc1df3359",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions['generated_text']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e931f-fb93-4de4-b7f9-be4a8ccaf4e8",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's use these functions to query the endpoint with our example texts and print the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddaeefc-95e6-4a41-943f-d54c942c2c47",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate to German:  My name is Arthur\n",
      "generated text: \u001b[1mIch bin Arthur.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: A step by step recipe to make bolognese pasta:\n",
      "generated text: \u001b[1mIn a large saucepan, combine the ground beef, onion, garlic, tomato paste, tomato\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in [text1, text2]:\n",
    "    query_response = query_endpoint(text.encode('utf-8'))\n",
    "    generated_text = parse_response(query_response)\n",
    "    print (f\"Inference:{newline}\"\n",
    "            f\"input text: {text}{newline}\"\n",
    "            f\"generated text: {bold}{generated_text}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2ad49-201f-4334-a27b-e62b9ba9f2c2",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Advanced Features\n",
    "\n",
    "The model we are using supports many advanced parameters that can be used to control the text generation process. These parameters include:\n",
    "\n",
    "- **max_length:** This parameter controls the maximum length of the generated text. The model will generate text until the output length (which includes the input context length) reaches `max_length`.\n",
    "- **num_return_sequences:** This parameter controls the number of output sequences returned by the model.\n",
    "- **num_beams:** This parameter controls the number of beams used in the greedy search during text generation.\n",
    "- **no_repeat_ngram_size:** This parameter ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence.\n",
    "- **temperature:** This parameter controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words.\n",
    "- **early_stopping:** If set to True, text generation is finished when all beam hypotheses reach the end of sentence token.\n",
    "- **do_sample:** If set to True, the model will sample the next word as per the likelihood.\n",
    "- **top_k:** In each step of text generation, the model will sample from only the `top_k` most likely words.\n",
    "- **top_p:** In each step of text generation, the model will sample from the smallest possible set of words with cumulative probability `top_p`.\n",
    "- **seed:** This parameter can be used to fix the randomized state for reproducibility.\n",
    "\n",
    "We can specify any subset of these parameters when invoking the endpoint. In the next section, we will show an example of how to invoke the endpoint with these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faacecfc-a9d6-44c2-ae23-89888ddb54c4",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":\"Tell me the steps to make a pizza\", \"max_length\":50, \"num_return_sequences\":3, \"top_k\":50, \"top_p\":0.95, \"do_sample\":True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de10c2-ff8a-4d04-b51e-c57ebb4237b6",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now define a function to query the endpoint with a JSON payload. This function will take the encoded JSON as input and return the response from the endpoint.\n",
    "\n",
    "The `query_endpoint_with_json_payload` function is similar to the `query_endpoint` function we defined earlier. The difference is that this function takes a JSON payload as input instead of a text. This allows us to pass the advanced parameters to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e974115-528c-4520-80bd-df98a531efde",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab43614-a39c-42f8-8301-a260b61a60df",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will also define a function to parse the response from the endpoint when multiple texts are returned. This function will extract the generated texts from the response.\n",
    "\n",
    "The `parse_response_multiple_texts` function is similar to the `parse_response` function we defined earlier. The difference is that this function extracts the 'generated_texts' field from the JSON instead of the 'generated_text' field. This is because when we request multiple texts from the endpoint, the response contains a 'generated_texts' field with a list of generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf4a45b-c164-463b-b372-b3fb0b0f0594",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions['generated_texts']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7420d-affe-4f44-b67d-074e459b26eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's use these functions to query the endpoint with our JSON payload and print the generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7354401b-c3b1-40b0-ac61-5d4068d965e1",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Place the pizza dough on a floured surface. Spread the pizza sauce on the dough. Spread the cheese on the pizza dough. Place the pizza on a baking sheet. Bake for 15 minutes at 450 degrees.', 'To make a pizza, you will first need to gather your ingredients. You will need a pizza pan, a large baking sheet, and a large pizza stone. You will also need to gather a large amount of toppings', 'Spread pizza sauce on dough. Top with cheese. Bake at 450 degrees for 20 minutes. Remove from oven and cut into slices.']\n"
     ]
    }
   ],
   "source": [
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b683e3-0ede-4205-a63d-57b63850a270",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Before we proceed to the next steps, let's ensure that we have the necessary libraries installed. We will need the `langchain` library for the following steps. If it's not already installed, we can install it using pip.\n",
    "\n",
    "The `langchain` library is a Python library that provides utilities for working with large language models. It includes utilities for creating prompts, querying endpoints, parsing responses, and more. We will use this library in the following steps to interact with our SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c0efe3-428d-4e12-a38e-b3671ac24736",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63738465-b0bd-48f3-b436-66c5e580b1eb",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's import the necessary modules from the `langchain` library.\n",
    "\n",
    "- `PromptTemplate`: This class is used to create a template for the prompts that we will pass to the language model. \n",
    "- `SagemakerEndpoint`: This class is used to interact with the SageMaker endpoint.\n",
    "- `LLMContentHandler`: This class is used to handle the content that we send to and receive from the language model.\n",
    "- `load_qa_chain`: This function is used to load a question-answering chain. A chain is a sequence of transformations applied to the input to generate an answer.\n",
    "- `Document`: This class is used to create documents that the language model can use to find the answer to a question.\n",
    "- `EmbeddingsContentHandler`: This class is used to handle the content that we send to and receive from the embedding model.\n",
    "- `SagemakerEndpointEmbeddings`: This class is used to interact with the SageMaker embeddings enpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba22d63-2d2a-462d-a39e-88b5d2a6d1f6",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dffc3-227c-4269-94a1-b836d8e7ffd1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now create a content handler for the language model to transform input to a format that the SageMaker endpoint expects and output to a form that the language model class expects. We will also define some parameters for the model.\n",
    "\n",
    "The `ContentHandler` class is a subclass of the `LLMContentHandler` class. It defines two methods:\n",
    "\n",
    "- `transform_input`: This method takes a prompt and a dictionary of model parameters as input, and returns the input in a format that the SageMaker endpoint expects. In this case, it converts the input to a JSON string and encodes it to bytes.\n",
    "- `transform_output`: This method takes the output from the SageMaker endpoint and returns it in a form that the language model class expects. In this case, it decodes the output from bytes to a string, parses the JSON, and returns the 'generated_texts' field.\n",
    "\n",
    "The `parameters` dictionary defines the parameters that we will use when querying the language model. These parameters control the behavior of the language model, such as the maximum length of the generated text, the number of sequences to return, and the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d01c2de-8ab3-4c2c-bd6e-a69664744287",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json['generated_texts'][0]\n",
    "\n",
    "llm_content_handler = ContentHandler()\n",
    "sm_llm=SagemakerEndpoint(\n",
    "            endpoint_name=endpoint_name,\n",
    "            region_name=\"eu-west-1\",\n",
    "            model_kwargs=parameters,\n",
    "            content_handler=llm_content_handler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f130c-b368-4562-b863-6d1f5062cd3d",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will define a prompt template and load a question answering chain. \n",
    "\n",
    "The prompt template is used to format the input to the language model. It accepts a set of parameters from the user that can be used to generate a prompt for a language model. \n",
    "\n",
    "The question answering chain is a sequence of transformations applied to the input to generate an answer.\n",
    "\n",
    "The `PromptTemplate` class takes a template string and a list of input variables as arguments. The template string is a string that contains placeholders for the input variables. The placeholders are enclosed in curly braces `{}` and correspond to the names of the input variables. When we use the prompt template, we will replace the placeholders with the actual values of the input variables.\n",
    "\n",
    "The `load_qa_chain` function loads a question answering chain. A chain is a sequence of transformations applied to the input to generate an answer. Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. In this case, the chain includes the language model and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc35a925-898d-46f3-bab7-fefd896983fc",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "            template=\"Use the following pieces of context to answer the question at the end.\\n{context}\\nQuestion: {question}\\nAnswer:\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "chain = load_qa_chain(\n",
    "        llm=sm_llm,\n",
    "        prompt=prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afa2f4-e7c3-4eb7-8095-6c8ebd9e5ebe",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, let's test our question answering chain with a sample question and some context. The context is a list of documents that the model can use to find the answer to the question.\n",
    "\n",
    "The `chain` function takes a dictionary as input and returns the output of the chain. The input dictionary must contain the 'input_documents' and 'question' keys. The 'input_documents' key corresponds to a list of documents that the model can use to find the answer to the question. The 'question' key corresponds to the question that we want to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a267d2db-d36a-4d70-a977-d4c26e189db2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '(ii).'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "input_documents = [Document(page_content=\"How can I help you?\")]\n",
    "\n",
    "chain({\"input_documents\": input_documents, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06cb55-e53c-4a89-85a3-ff8128b4a6a1",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will create a content handler for embeddings to transform a format that the SageMaker endpoint expects and output to a form that the embeddings class expects.\n",
    "\n",
    "The `SagemakerEndpointEmbeddingsJumpStart` class is a subclass of the `SagemakerEndpointEmbeddings` class. It defines the `embed_documents` method, which computes document embeddings using a SageMaker Inference Endpoint. The method takes a list of texts and a chunk size as input, and returns a list of embeddings.\n",
    "\n",
    "The `ContentHandler` class is a subclass of the `EmbeddingsContentHandler` class. It defines two methods:\n",
    "\n",
    "- `transform_input`: This method takes a prompt and a dictionary of model parameters as input, and returns the input in a format that the SageMaker endpoint expects. In this case, it converts the input to a JSON string and encodes it to bytes.\n",
    "- `transform_output`: This method takes the output from the SageMaker endpoint and returns it in a form that the embeddings class expects. In this case, it decodes the output from bytes to a string, parses the JSON, and returns the 'embedding' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb0f7dc1-6584-4c9e-a2bb-30e465881cf2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "embeddings_content_handler=ContentHandler()\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=\"eu-west-1\",\n",
    "    content_handler=embeddings_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72fe226-dd98-4e1b-95fd-08ff5f79bae6",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Now, we will download the data we will use for our documents from an S3 bucket and process it for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d747e2b-2aba-4be4-a185-3c7ea1d873f8",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to rag_data/Amazon_SageMaker_FAQs.csv\n"
     ]
    }
   ],
   "source": [
    "original_data = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/\"\n",
    "!mkdir -p rag_data\n",
    "!aws s3 cp --recursive $original_data rag_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffb696-ee06-4a28-8ca3-3ed3886afb92",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now load the data into a pandas DataFrame and process it for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f8b4227-37fc-44e7-9371-8283087e0be5",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer\n",
       "0  Amazon SageMaker is a fully managed service to...\n",
       "1  For a list of the supported Amazon SageMaker A...\n",
       "2  Amazon SageMaker is designed for high availabi...\n",
       "3  Amazon SageMaker stores code in ML storage vol...\n",
       "4  Amazon SageMaker ensures that ML model artifac..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "all_files = glob.glob(os.path.join(\"rag_data/\", \"*.csv\"))\n",
    "\n",
    "df_knowledge = pd.concat(\n",
    "    (pd.read_csv(f, header=None, names=[\"Question\", \"Answer\"]) for f in all_files),\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "df_knowledge.drop([\"Question\"], axis=1, inplace=True)\n",
    "df_knowledge.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852de1c-0710-4ca6-bdce-5d3221b3455c",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now save the processed data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728943d5-f4bd-4088-9a8d-08c538e591af",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "df_knowledge.to_csv(\"rag_data/processed_data.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879915e9-6ef5-41cd-a986-3abf044ec7c8",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Next, we will load the processed data using a CSVLoader. The CSVLoader is a utility that helps us load data from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b8361d0-c950-4bc7-b2c9-a8d16e9515e2",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path=\"rag_data/processed_data.csv\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7305ce-64a7-493e-a816-930c443b6670",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now install the `faiss-cpu` library, which provides efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI that allows for efficient similarity search and clustering of dense vectors. So, given a set of vectors(in this case a vector representation of a document i.e. an embedding), we can index them using Faiss â€” then using another vector (the query vector), we search for the most similar vectors within the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c7fb7ce-54a3-4118-bfa7-32f79192f222",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cb248-4acb-4b4d-adbd-54e2f4ec9997",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "We will now create an index of our documents using the VectorstoreIndexCreator. This index will allow us to perform efficient similarity searches on our documents.\n",
    "\n",
    "The VectorstoreIndexCreator is a utility that helps us create an index of our documents. It uses the embeddings of the documents to create the index. The embeddings are dense vectors that represent the documents. The index allows us to perform efficient similarity searches on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f96ef5-4026-49fc-a0a0-4aaa1f2e9395",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=300, chunk_overlap=0),\n",
    ")\n",
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcac91-176b-4cf3-8f6b-eeb8cd469751",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Let's test our index by querying it with a sample question.\n",
    "\n",
    "The `index.query` function is used to perform a similarity search on the index. It takes a question and a language model as input, and returns the most similar documents in the index. After the relevant documents are retrieved, the LLM can be used to generate a coherent and contextually relevant answer based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "175c4fc3-a0e9-4379-9832-9e5ebfae187a",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon EC2 Spot instances'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(question=query, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee42e36-d687-45f1-b0d5-befdf5c16f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will now replicate the index.query functionality step by step to illustrate what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a07a4-5044-4639-8351-be6399eabf53",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "we will create a document search object using the FAISS vector store and our documents. This will allow us to perform similarity searches on our documents. Using this we retrieve the top 3 most similar docs to our query.\n",
    "\n",
    "The `FAISS.from_documents` function is used to create a FAISS vector store from our documents. The embeddings of the documents are used to create the vector store. The vector store allows us to perform efficient similarity searches on the documents.\n",
    "\n",
    "The `docsearch.similarity_search` function is used to perform a similarity search on the documents. It takes a query and a number of results to return as input, and returns the most similar documents in the vector store. The query is converted into an embedding and this embedding is then compared with the embeddings of the documents in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a43072c-1102-400b-b5f7-d4a52b8d614f",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Once a Managed Spot Training job is completed, you can see the savings in the AWS Management Console and also calculate the cost savings as the percentage difference between the duration for which the training job ran and the duration for which you were billed. Regardless of how many times your Managed Spot Training jobs are interrupted, you are charged only once for the duration for which the data was downloaded.', metadata={'source': 'rag_data/processed_data.csv', 'row': 88}),\n",
       " Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Managed Spot Training uses Amazon EC2 Spot instances for training, and these instances can be pre-empted when AWS needs capacity. As a result, Managed Spot Training jobs can run in small increments as and when capacity becomes available. The training jobs need not be restarted from scratch when there is an interruption, as Amazon SageMaker can resume the training jobs using the latest model checkpoint. The built-in frameworks and the built-in computer vision algorithms with SageMaker enable periodic checkpoints, and you can enable checkpoints with custom models.', metadata={'source': 'rag_data/processed_data.csv', 'row': 86}),\n",
       " Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: If you have a consistent amount of Amazon SageMaker instance usage (measured in $/hour) and use multiple SageMaker components or expect your technology configuration (such as instance family, or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings while providing flexibility to change the underlying technology configuration based on application needs or new innovation. The Savings Plans rate applies automatically to all eligible ML instance usage with no manual modifications required.', metadata={'source': 'rag_data/processed_data.csv', 'row': 149})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch = FAISS.from_documents(documents, embeddings)\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffb027-7392-4e41-9fcc-e1b87e96e7ea",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "Finally, we will use our question-answering chain to answer our query using the documents we found.\n",
    "\n",
    "The `chain` function is used to apply our question-answering chain to our query and documents. The question-answering chain is a sequence of transformations that are applied to the query and documents to generate an answer. The transformations include converting the query and documents into embeddings, performing a similarity search on the embeddings, and generating an answer from the most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65db642c-0143-4adb-8b61-4f556966bb76",
   "metadata": {
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Spot instances'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec624fcc-68b6-428d-950d-600ff5336a91",
   "metadata": {},
   "source": [
    "We have looked at two flows so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96249b-a493-475f-b239-1aed1025947c",
   "metadata": {},
   "source": [
    "a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144f57a-8c3f-425e-bb66-91897c60b301",
   "metadata": {},
   "source": [
    "![alt text](flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda94971-ef3f-433c-87f9-d1ccbd2d0174",
   "metadata": {},
   "source": [
    "b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b173f-7078-4e73-abd1-e4d0f5719571",
   "metadata": {},
   "source": [
    "![alt text](RAGflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08651a-993e-4b69-99a8-5b25451a50a9",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this notebook, you should clean up your AWS resources to avoid any unwanted charges. This includes deleting the SageMaker endpoint. [add cleanup steps]"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "noteable": {
   "last_transaction_id": "07d0066e-b9dd-4343-87d9-2e007e4a83d1"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "0070049a-0f1b-5328-927c-3f92cabf9583",
    "openai_ephemeral_user_id": "6f4539cc-18e6-5a48-a00c-a8fe1e1c005a"
   }
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
